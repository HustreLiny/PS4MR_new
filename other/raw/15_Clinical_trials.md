# 15 Clinical trials

In a controlled trial, as in all experimental work, there is no need for the search for precision to throw sense out of the window.

Hill (1963)


## 15.1 INTRODUCTION

A clinical trial is a planned experiment on human beings which is designed to evaluate the effectiveness of one or more forms of treatment. Trials can be carried out to evaluate anything that may be considered a potential treatment in its widest sense, such as drugs, surgical procedures, physiotherapy, diet, acupuncture, health education, and so on. I shall use the term clinical trial to refer to any such study.

Clinical trials merit special attention because of their medical importance, some particular problems in design and analysis, and certain ethical problems. The methodology that is used was introduced into medical research about 50 years ago, with the most famous early example being a trial comparing streptomycin and bed rest with bed rest alone in the treatment of pulmonary tuberculosis (MRC, 1948). Comparative clinical trials were virtually unknown before the 1940s. Pocock (1983, p. 14) gives a summary of the historical development of clinical trials.

Within the pharmaceutical industry clinical trials are classified into one of four categories:

1. Phase I: Clinical pharmacology and toxicity;

2. Phase II: Initial clinical investigation;

3. Phase III: Full scale evaluation of treatment;

4. Phase IV: Postmarketing surveillance.

In this Chapter I shall consider only Phase III trials. They have the distinguishing feature that they involve direct comparison between two or more treatments. They are often referred to as comparative trials or controlled trials. Although some controlled trials are set up to compare more than two treatments I shall concentrate on the common two group case. I shall usually consider the two treatments to be an experimental treatment, perhaps a new drug, and a control treatment, which may be a

standard treatment, a placebo, or even no treatment at all, depending on circumstances.

In practice the vast majority of comparative clinical trials have certain features in common which makes it possible to give general guidance on design, analysis and interpretation. Perhaps for this reason, clinical trials are probably the area of medical research where the integration of statistical ideas and methodology has been most successful.

The key idea of a clinical trial is that we wish to compare groups of patients who differ only with respect to their treatment. If the groups differ in some other way then the comparison of treatments is biased. If we can identify a bias then it may be possible to allow for its effect in the analysis, but unknown biases cannot be dealt with. The methods of design and analysis described in this chapter are aimed at the elimination of bias.

Deeper consideration of the issues covered in this chapter, as well as topics not covered here, can be found in several books devoted to clinical trials, of which that by Pocock (1983) is particularly recommended. In addition, the papers by Peto et al. (1976) and Pocock (1985) discuss some of the trickier issues. Lastly, much wisdom can be found in the chapter on clinical trials in the famous book by Bradford Hill (1984).


## 15.2 DESIGN OF CLINICAL TRIALS


### 15.2.1 The need for a comparison group

The introduction of a new treatment is a long and complex affair, and many apparently promising therapies fall by the wayside. It is natural to begin investigation by trying a new treatment on some patients to see what happens. This type of study is uncontrolled, so that any benefits or harmful effects seen in the patients will naturally be ascribed solely to the treatment. Such studies are usually open, where the clinician and the patients know what treatment each patient is getting. The investigator's natural enthusiasm for the new treatment may well influence his judgement of the patients' progress, and may also be transmitted to the patients and affect their well- being, especially for conditions where symptoms are subjective, such as degree of pain. Many early studies of this type have suggested that new treatments were highly effective, only for this apparent benefit to disappear on more careful examination. In some cases early results may lead to a treatment being adopted without what we would now consider to be adequate investigation. There are several instances of treatments being investigated after many years' clinical use and being found ineffective. One such was gastric freezing as a treatment for duodenal ulcer, which was discovered, adopted and abandoned within the space of seven years (Miao, 1977). A particularly marked example is the story of the epidemic in babies of retrolental fibroplasia leading to

blindness. In the 1950s high doses of oxygen were given to very premature babies. However, the treatment of infants with early eye changes with adrenocorticotrophic hormone had a  $75\%$  success rate. Both the oxygen and hormone treatments had been adopted without the benefit of controlled trials. Only after several years of clinical use was it found, after clinical trials were belatedly carried out, that the hormone treatment was ineffective -  $75\%$  of such infants return to normal without treatment - and that the oxygen treatment was positively harmful; it caused the blindness in the first place (Silverman, 1985).

There is a place for uncontrolled experiments, designated above as Phase II trials, but they tend to give over- optimistic, and hence biased, results. Definitive assessment of a new treatment should be in relation to the effectiveness of an alternative treatment.

As we will see, there are major advantages if the two treatments are investigated concurrently, allocation of treatments to patients is by a random process, and neither the patient nor the clinician knows which treatment was received. The randomized double- blind controlled trial is usually taken as the 'gold standard' against which to judge the quality of the design of a trial.


### 15.2.2 Random allocation

A vital issue in design is to ensure that the allocation of treatments to patients is independent of the characteristics of the patients - in other words, it is carried out in an unbiased way. The most widely used method of unbiased treatment allocation is to use random allocation to determine which treatment each patient gets. As we saw in Chapter 5, random allocation gives all subjects the same chance of receiving either treatment and is thus unbiased by definition. Another important reason for using random sampling is that statistical methods of analysis are based on what we expect to happen in random samples from populations with specified characteristics.

It is highly desirable that, as far as is possible, the groups of patients receiving the different treatments are very similar with regard to features that may affect how well they do, that is in their prognosis. For example. in most studies it is important that the age distribution of the groups similar, because prognosis is very often related to age. There is a guarantee, however, that randomization will in fact lead to the groups being very similar. Any differences between the groups will have arise a chance, but such differences can be at least inconvenient, and may lead to doubts being cast on the interpretation of the trial results. While it s possible to modify the analysis to take account of any differences between the groups at the start (see section 15.4), it is far better to try to control the problem at the design stage. Most obviously this can be done by using

stratified randomization, as described in section 5.7.3. If we know in advance that there are a few key variables that are strongly prognostic then they can be incorporated into a stratified randomization scheme. As observed in section 5.7.3, it is essential that stratified randomization uses blocking, otherwise there is no benefit over simple randomization. There may well be other important variables that we cannot measure or have not identified, and we must rely on the randomization to balance them out. The benefits of having a stratified design are not widely accepted (Peto et al., 1976; Meier, 1981), especially as the increased complexity gives more scope for errors in execution.A different method of obtaining well- matched groups is to use the technique of minimization described in the next section.

A different method of obtaining well- matched groups is to use the technique of minimization described in the next section.


### 15.2.3 Minimization

The desirability of random allocation in comparative studies was stressed in the previous section. The use of non- random controls in clinical trials severely lessens the credibility of the results.

Minimization is one non- random method, however, that can be used safely. Indeed, it has definite advantages over both simple or stratified random sampling, unless the sample size is large. The use of minimization will provide treatment groups very closely similar for several variables, even in small samples. It is especially suitable for smaller trials and for trials where small numbers of patients are recruited from each of several centres.

Table 15.1 Some baseline characteristics of patients in a controlled trial of mustine versus talc in the control of pleural effusions in patients with breast cancer (Fentiman et al., 1983)  

<table><tr><td rowspan="2"></td><td colspan="2">Treatment</td></tr><tr><td>Mustine (n = 23)</td><td>Talc (n = 23)</td></tr><tr><td>Mean age (SE)</td><td>50.3 (1.5)</td><td>55.3 (2.2)</td></tr><tr><td>Stage of disease:</td><td></td><td></td></tr><tr><td>1 or 2</td><td>52%</td><td>74%</td></tr><tr><td>3 or 4</td><td>48%</td><td>26%</td></tr><tr><td>Mean interval in months between breast cancer diagnosis and effusion diagnosis (SE)</td><td>33.1 (6.2)</td><td>60.4 (13.1)</td></tr><tr><td>Postmenopausal</td><td>43%</td><td>74%</td></tr></table>

Table 15.1 shows some characteristics of breast cancer patients randomized to receive either mustine or talc as a treatment for pleural effusions. Simple randomization was used in the small trial, and by chance the two treatment groups were noticeably different. Stratified randomization would have helped, but it is not feasible to stratify on several variables in such a small trial. With minimization the two groups would have been very similar with respect to all of these variables, and the results would have been more convincing.

Minimization is based on a completely different principle from randomization. If we regard the patients for the trial as arriving one at a time, then the first patient is given a treatment at random. For each subsequent patient we determine which treatment would lead to better balance between the groups with respect to the variables of interest. The patient is then randomized using a weighting (see section 5.7.1) in favour of the treatment which would minimize the imbalance. For example, we might use a weighting of 4 to 1, so that there is an  $80\%$  chance of each patient getting the treatment that minimizes the imbalance. The effect of this procedure is that the groups will be much more similar with regard to the chosen variables than they would be with simple randomization.

Suppose that the mustine vs talc trial had used minimization based on the four variables shown in Table 15.1. For each variable we can divide the possible values into two groups, as follows:

Age (years)  $\leq 50$  or  $>50$  Stage of disease 1 or 2 or 3 or 4 Time between diagnosis of cancer and  $\leq 30$  or  $>30$  diagnosis of effusions (months) Menopausal status Pre or Post

Suppose that after 29 patients had entered this trial the numbers in each subgroup in each treatment group were as shown in Table 15.2. We now wish to enter into the trial a patient with the following characteristics: 57 years old; stage 3; time interval 22 months; postmenopausal. The numbers of women with this patient's characteristics already in the two treatment groups are shown in Table 15.3. As we wish to have the two groups as similar as possible, the preferable treatment for the new patient is that with the smaller total. Here we would use weighted randomization with a weighting in favour of talc.

After the patient is allocated to a treatment the numbers in each group are updated and the process is repeated for the next patient. If for any patient the totals for the two treatments are the same, then the choice should be made using simple (unweighted) randomization, as it is for the first patient. The method extends simply to variables with more than two categories and to trials of more than two treatments.

Table 15.2 Characteristics of the first 29 patients in a clinical trial using minimization to allocate treatments  

<table><tr><td></td><td></td><td>Mustine 
(n = 15)</td><td>Talc 
(n = 14)</td></tr><tr><td rowspan="2">Age</td><td>≤ 50</td><td>7</td><td>6</td></tr><tr><td>&amp;gt; 50</td><td>8</td><td>8</td></tr><tr><td rowspan="2">Stage</td><td>1 or 2</td><td>11</td><td>11</td></tr><tr><td>3 or 4</td><td>4</td><td>3</td></tr><tr><td rowspan="2">Time interval</td><td>≤ 30 m</td><td>6</td><td>4</td></tr><tr><td>&amp;gt; 30 m</td><td>9</td><td>10</td></tr><tr><td rowspan="2">Menopausal status</td><td>Pre</td><td>7</td><td>5</td></tr><tr><td>Post</td><td>8</td><td>9</td></tr></table>

Table 15.3 Calculation of imbalance in patient characteristics for allocating treatment to the thirtieth patient  

<table><tr><td></td><td></td><td>Mustine 
(n = 15)</td><td>Talc 
(n = 14)</td></tr><tr><td>Age</td><td>&amp;gt; 50</td><td>8</td><td>8</td></tr><tr><td>Stage</td><td>3 or 4</td><td>4</td><td>3</td></tr><tr><td>Time interval</td><td>≤ 30 m</td><td>6</td><td>4</td></tr><tr><td>Postmenopausal</td><td></td><td>8</td><td>9</td></tr><tr><td></td><td></td><td>Total 26</td><td>24</td></tr></table>

The random component can be omitted from the allocation of treatments, so that each patient is automatically given the treatment which leads to less imbalance. Although the treatment that a particular patient receives depends in a complicated way upon the characteristics of the patients already entered into the trial, the absence of a random element introduces a small possibility of selection bias. It is preferable therefore to use weighted randomization.

Minimization is a valid alternative to ordinary randomization, and it has the important advantage, especially in small trials, that there will be only minor differences between the groups with respect to those variables used in the allocation process. It is particularly suitable to be performed with the aid of a computer program, but it is not difficult to perform 'by hand' if the record of the numbers of patients with each characteristic in each group is updated after each new patient has entered the trial.


### 15.2.4 Other methods of treatment allocation

Alternatives to random allocation may be divided into systematic (or pseudo- random) methods and non- random methods. Non- randomized trials can be further divided into those with concurrent or non- concurrent (or historical) controls.


#### (a) Systematic allocation  <!-- 非数字标题，四级 -->

A common approach is to allocate treatments to patients according to the patient's date of birth or date of enrolment in the trial (such as giving treatment A to those with even dates, and treatment B to those with odd dates), by the terminal digit of the hospital number, or simply alternately into the different treatment groups. While all of these approaches are in principle unbiased, problems arise from the openness of the allocation system. Put crudely, it is a well- known phenomenon for the allocation to be altered by someone with access to the procedure. Further, knowledge of which treatment a patient is destined to receive can affect the decision about whether to enter that patient into the trial. While such actions are often taken for altruistic motives, the result is a biased allocation and quite possibly a worthless set of data.

Although systematic allocation appears unbiased, it is open to abuse and cannot be recommended unless there really is no alternative. The term 'pseudo- random' is misleading, as there is no random element and the method is definitely inferior to true random allocation.


#### (b) Non-random concurrent controls  <!-- 非数字标题，四级 -->

The use of non- random controls leads to problems of interpretation, because it will usually be impossible to establish that the groups are comparable. Indeed, the groups may specifically differ in known ways but with unknown effect. For example, in the trial of vitamin supplementation versus placebo in relation to neural tube defects (Smithells et al., 1980). discussed further below, the control group included women ineligible for the trial as well as women who refused to participate. Many studies have shown that there is a volunteer bias, with volunteers usually having a better prognosis than refusers. We should worry about bias whenever there is a systematic difference between the patients given different treatments, for example when the groups are taken from patients at different hospitals. Studies where the treatments are given as deemed appropriate by the clinician are especially unreliable.


#### (c) Historical controls  <!-- 非数字标题，四级 -->

Probably the simplest approach to evaluating a new treatment is to compare a single group of patients all given the new treatment with a group previously treated with an alternative treatment. Often these will be two consecutive series of patients in the same hospital(s). Despite a few

advocates, this approach is seriously flawed as we can never satisfactorily eliminate possible biases due to other factors that may have changed over time. Pocock (1977) showed that in 19 cases where the same therapy was used in two consecutive trials of cancer chemotherapy in the same institution there were large changes in the observed death rates, ranging from  $- 46\%$  to  $+24\%$ . While some of the variation was probably due to small sample sizes, four of the differences were statistically significant at the  $2\%$  level. Sacks et al. (1983) compared trials of the same therapies in which randomized or historical controls were used, and found a consistent tendency for historically controlled trials to yield more optimistic results than randomized trials. The use of historical controls can only be justified in tightly controlled situations of relatively rare conditions, such as in evaluating therapies for advanced cancer.

The balance of opinion has now swung so far towards randomized trials that the results of non- randomized trials may cause major controversy. A recent example was the study of the possible benefit of vitamin supplementation at the time of conception in women at high risk of having a baby with a neural tube defect (NTD) (Smithells et al., 1980). They found that the vitamin group subsequently had fewer NTD babies than the placebo control group, but because the study was not randomized the findings are not widely accepted, and the Medical Research Council is now running a large randomized trial to try to get a proper answer to the question.


### 15.2.5 Alternative designs

The simplest design for a clinical trial is called the parallel group design, in which two different groups of patients are studied concurrently. This is the design that has been implicit in this chapter so far. The most common alternative is the crossover design, which is described below together with some other less common designs that are worth knowing about.


#### (a) Crossover design  <!-- 非数字标题，四级 -->

A crossover trial is one in which the same group of patients are given both (or all) treatments of interest in sequence. Here randomization is used to determine the order in which the treatments are received. The crossover design has some attractive features, in particular that the treatment comparison is 'within- subject' rather than 'between- subject', and that the sample size needed is smaller. There are some important disadvantages, however, which I shall describe in relation to a two- period crossover trial:

1. Patients may drop out after the first treatment, and so not receive the second treatment. Withdrawal may be related to side-effects. Treatment periods should be fairly short to minimize the risk of drop-out for other reasons.

2. There may be a carry-over of treatment effect from one period to the next, so that the results obtained during the second treatment are affected by what happened in the first period. In other words, the observed difference between the treatments will depend upon the order in which they were received. In the presence of such a treatment-period interaction the data for the second period may have to be discarded, severely weakening the power of the trial.

3. There may be some systematic difference between the two periods of the trial. For example, the observations in the second period may be somewhat lower than those in the first period, regardless of treatment. A small period effect is not too serious, as it applies equally to both treatments.

4. Crossover studies cannot be used for conditions which can be cured, and are most suitable when the effect of the treatment can be assessed quickly.

It is desirable to establish in advance that there will not be any carry- over treatment effect, but the information may be unavailable. A wash- out period is sometimes introduced between the treatment periods to try to eliminate carry- over effects. Because of the problems described, crossover studies are probably overused. Further discussion is given by Woods et al. (1989).

The analysis of crossover trials is explained and illustrated in section 15.4.10.


#### (b) Within group (paired) comparisons  <!-- 非数字标题，四级 -->

Another type of within group design is when alternative treatments are investigated in the same subjects at the same time. It can be used for treatments that can be given independently to matching parts of the anatomy, such as limbs or eyes. The matched design has all the advantages of the crossover design, but none of the disadvantages, so is a very powerful design. Unfortunately, there are few circumstances in which it can be used.

The nearest equivalent to the paired within subject design is the matched pairs design, where pairs of subjects are matched for, say, age, sex and certain prognostic factors, and the two treatments are then allocated to the pair of subjects at random. This design can only be used easily when there is a pool of subjects that can be entered into the trial, in order to be able to find matched pairs. Where there are known important prognostic variables the design removes much of the between subject variation, and ensures that the subjects receiving each treatment have very similar characteristics.


#### (c) Sequential designs  <!-- 非数字标题，四级 -->
Another type of design is the sequential trial, in which parallel groups are

studied, but the trial continues until a clear benefit of one treatment is seen or it is unlikely that any difference will emerge. The main advantage of sequential trials is that they will be shorter than fixed length trials when there is a large difference in the effectiveness of the two treatments.

In sequential trials the data are analysed after each patient's results become available. Their use is therefore restricted to conditions where the outcome is known relatively quickly. There are problems with blinding (see section 15.2.6), and possibly also ethical difficulties.

A useful variation on this principle is the group sequential trial, in which the data are analysed after each block of patients has been seen, perhaps four or five times in all. This allows the trial to be planned more easily (regarding length) but also enables the trial to be stopped early if a clear treatment difference is seen.

In the right circumstances sequential trials are a good method, and they should be used more frequently.


#### (d) Factorial designs  <!-- 非数字标题，四级 -->

One further type of design is called the factorial design, in which two treatments, say A and B, are simultaneously compared with each other and with a control. Patients are divided into four groups, who receive the control treatment, A only, B only, and both A and B. This design allows the investigation of the interaction (or 'synergy') between A and B. The factorial design is rarely used in clinical trials, but Pocock (1983, p. 139) describes some examples of its use.


#### (e) Adaptive designs  <!-- 非数字标题，四级 -->

Ethical considerations have led some people to advocate adaptive designs, in which the proportion of subjects getting the inferior treatment diminishes as the trial proceeds. In other words, a patient's treatment depends to some extent on the outcome of treatment in previous patients in the trial. Apart from practical difficulties, such as needing to know quickly the results from each patient, it is questionable whether this design resolves any ethical problems. Adaptive designs have rarely been used.


#### (f) Zelen's design  <!-- 非数字标题，四级 -->

Lastly, Zelen (1979) proposed a variation on the randomized trial that seems to avoid problems associated with getting informed consent. Half of the subjects are allocated at random to receive the standard treatment, and are treated as if they were not in a trial. The other half are offered the new experimental treatment, but they can choose to have the standard treatment if they wish. An essential feature of Zelen's proposal (Zelen, 1979) is that the two groups are analysed as originally randomized, regardless of which treatment those in the second group actually opted for. While this design has some useful features, it can only be of value when a high proportion of those offered the new treatment take it, which cannot

be known in advance. This design has rarely been used, and many consider it unethical not to tell half the patients that they are in a trial. A variation is where both groups are told which treatment they have been allocated and are offered the chance to switch to the other. While resolving the ethical difficulty, there are possible difficulties associated with the necessary lack of blindness and loss of power if too many patients opt to change treatment. There does not seem to be much to recommend this design. Ellenberg (1984) gives further discussion.


### 15.2.6 Blindness

The key to a successful clinical trial is to avoid any biases in the comparison of the groups. Randomization deals with possible bias at the treatment allocation, but bias can also creep in while the study is being run. Both the patient and the doctor may be affected in the way they respectively respond and observe by knowledge of which treatment was given. For this reason, it is desirable that neither the patient nor the person evaluating the patient knows which treatment was given. Such a trial is called double- blind. If only the patient is unaware, as is sometimes the case, the trial is called single- blind. In several fields, such as surgery, it is often impossible for a study to be double- blind. Clinical trials should use the maximum degree of blindness that is possible.

In addition, the treatment allocation system should be set up so that the person entering patients does not know in advance which treatment the next person will get. A common way of doing this is to use a series of consecutively numbered sealed opaque envelopes, each containing a treatment specification. For stratified randomization, two or more sets of envelopes are needed. For drug trials the allocation may be carried out by the pharmacy, who will produce numbered bottles which do not indicate the treatment contained.

Double- blind trials clearly require that the different treatments should be indistinguishable to the patient and to whoever assesses the patient. For drug trials comparing two active treatments this may require the double dummy technique, in which each patient receives one of the active drugs and a dummy tablet that looks like the alternative active drug.


### 15.2.7 Placebos

When we wish to evaluate a new treatment for a condition there is the problem of what treatment to give to the control group. If (and only if) there is no existing standard beneficial treatment, then it is reasonable not to give the control group any active treatment. However, there are two reasons why it is desirable to give the control group patients an inert dummy or placebo treatment, rather than nothing. Firstly, the act of taking

some treatment may itself have some benefit to the patient, so that if we give nothing at all to the control group then part of any benefit observed in the treated group could be due to the knowledge or belief that they had taken a treatment. This is known as the placebo effect. Secondly, in order for a study to be double- blind it is necessary for the two treatments to be indistinguishable. Placebo tablets should therefore be identical in appearance and taste to the active treatment, but pharmacologically inactive.

Many clinical trials do, in fact, find some apparent benefit of treatment in the placebo group, and there are often side- effects too. Without a comparison group, who may be given an alternative active treatment or a placebo, we cannot know how specific any benefit (or harm) is to the new treatment being investigated. For example, if there are as many reported headaches in the active and placebo treated groups, we would not consider headache as a side- effect of the active treatment.

Placebos can sometimes be used in non- drug trials too. In section 10.3 I described a trial that had used mock electrical stimulation as a control treatment. Likewise a control for acupuncture is easily set up by having needles inserted at the 'wrong' points. There may however be ethical problems associated with invasive placebos.


### 15.2.8 Selection of subjects

Clinical trials are a prime example of the principle that we collect data from a sample and use the results of the analysis to make inferences about the population of all such subjects. In order for this process to work it is clearly necessary to select a representative sample. In practice, however, many restrictions are usually placed on who is eligible to take part in a trial, and so extrapolation of the results to the population may be difficult. For example, a placebo- controlled trial was carried out in British doctors to see if daily aspirin would reduce the incidence of and mortality from stroke, myocardial infarction and other vascular conditions (Peto et al., 1988). The investigators identified 20000 doctors who were willing to participate but almost three- quarters of them were ineligible, either because they were already taking aspirin for some reason, because there were reasons why they could not take aspirin, or because they had a history of peptic ulcer, stroke or myocardial infarction. The doctors who took part in the study were therefore a selected group of more healthy individuals.

Figure 15.1 shows how patients with either low or high blood pressure would be excluded from a trial of a new hypertensive treatment, although for different reasons. Often the patients whom it is both ethical and reasonable to include in a trial are those most likely to benefit if the treatment is effective. In general, but not always, we do not expect treatment to do much for patients who already have an excellent prognosis, nor for those with a dreadful prognosis.

![](../images/15_Clinical_trials/img1.jpg)  
Figure 15.1 Diagram showing the eligibility of patients for a trial of a new antihypertensive agent (based on Elwood, 1982).

Begg and Engstrom (1987) discussed the problem of over- restrictive eligibility criteria in cancer clinical trials, which can lead to most patients with a disease being ineligible for a trial. They suggest that many exclusion criteria are unnecessary. The more restrictive the exclusion criteria, the less generalizable will be the results of the trial. In large trials especially it is better not to be too restrictive, although in small trials there may be some advantage in keeping the study subjects more homogeneous, especially if simple randomization is used.


### 15.2.9 Ethical issues

A clinical trial is an experiment on human beings, so it is not surprising that there are several important ethical issues relating to clinical trials. One concerns the amount of information given to the patient. In general the patient should be invited to be in the trial, and should be told what the alternative treatments are (although they will usually not know which they will get). They can decline to be in the trial, in which case they will be treated normally. If they agree to participate they will often have to sign a form stating that they understand the trial. This informed consent is controversial, because it is likely that many patients do not really understand what they are told, and that they are not always told as much as they should be. There are some cases where it is not possible to get informed consent, for example when the patients are very young, very old, or unconscious. Also there are a few circumstances where it might be difficult to get people to agree to be randomized, such as in a trial comparing mastectomy with chemotherapy as a treatment for breast cancer.

From the clinical side, no doctor should participate in a clinical trial if he/she believes that one of the treatments being investigated is superior, and they should not enter any patient for which they think that a particular treatment is indicated. In other words, the ideal medical state to be in is one of ignorance: the trial is carried out because we do not know which treatment is better. It may be thought that an active treatment, which will have yielded promising results in uncontrolled observational studies, would be certain to be better than a placebo, but this is not always so. Further, even if the treatment is beneficial there may be unacceptable side- effects.

In many countries there are a large number of ethics committees set up to consider proposals to carry out clinical trials (and, indeed, any research involving human subjects). Interestingly, the problems relating to the design of the trial of vitamin supplementation around conception (Smithells et al., 1980) stemmed largely from the refusal of ethics committees to sanction the randomized trial that was originally proposed. The study as performed did, of course, have the approval of the ethics committees. Ethics committees are usually concerned only with the welfare of the patient, and do not consider scientific, including statistical, issues.

Regarding design, it can be argued that non- randomized trials, especially those with non- concurrent (historical) controls, are unethical because, as shown earlier, the results of such trials are so unreliable. Similar comments can be levelled at any trial which uses suboptimal methodology, although it is not possible to draw a precise line between ethical and unethical studies.

More generally, any study (not necessarily a clinical trial) that uses substandard statistical methods, especially in design or analysis, may be deemed unethical for three reasons (Altman, 1982a):

1. the misuse of patients by exposing them to unjustified risk and inconvenience;  
2. the misuse of resources, including the researchers' time, which could be better employed on more valuable activities; and  
3. the consequences of publishing misleading results, which may include the carrying out of unnecessary further work.

Many of the ethical issues relating to clinical trials were dealt with by Bradford Hill (1963). Silverman (1985, p. 153) gives a more recent review of the main issues.


### 15.2.10 Outcome measures

In most clinical trials information about the effect of treatment is gathered in relation to many variables, sometimes on more than one occasion. There is the temptation to analyse each of the variables and look to see which differences between treatment groups are significant. This approach leads to misleading results, because multiple testing will invalidate the results of

hypothesis tests. In particular, presenting only the most significant results, as if these were the only analyses performed, is fraudulent.

A preferable approach is to decide in advance of the analysis which outcome measure is of major interest, and focus attention on this variable when analysing the data. Other data can and should be analysed too, but these variables should be considered to be of secondary importance. Any interesting findings among the secondary variables should be interpreted rather cautiously, more as ideas for further research than as definitive results. Side- effects of treatment should be treated in this way.

Sometimes there really will be more than one major outcome measure. If there are two, then no great harm will come from analysing them both, perhaps taking a stricter cut- off for statistical significance. Sometimes it is possible to combine two variables into one, in particular when the variables of interest are alternative events, such as death or heart attack.

Finally, note that sample size calculations (see section 15.3) are based on a single variable.


### 15.2.11 Protocols

An important aspect of planning a clinical trial is to produce a protocol, which is a formal document outlining the proposed procedures for carrying out the trial.

Pocock (1983, pp. 28- 31) suggests the following main features of a study protocol:

1. background and study objectives

2. specific objectives

3. patient selection criteria

4. treatment schedules

5. methods of patient evaluation

6. trial design

7. registration and randomization of patients

8. patient consent

9. required size of study

10. monitoring of trial progress

11. forms and data handling

12. protocol deviations

13. plans for statistical analysis

14. administrative responsibilities.

A protocol is necessary when applying for a grant to carry out a trial. and most of the above information will be required by the local ethics committee. Further, as well as aiding in the carrying out of a trial. a protocol makes the writing up of the results much easier as the introduction and methods section of the paper should be substantially the same as sections 1 to 9 above.

For multicentre studies a detailed protocol is essential, and it is strongly recommended for any clinical trial. Indeed, I recommend the drawing up of a proper protocol for any research project - most of the above categories are not specific to clinical trials.


## 15.3 SAMPLE SIZE


### 15.3.1 Introduction

In section 8.5.3 I introduced the concept of power in relation to hypothesis testing. The power of a test is the probability that a study of a given size would detect as statistically significant a real difference of a given magnitude. The medical literature contains many trials that were far too small to have a good chance of detecting clinically worthwhile differences between the treatments being investigated. It is clear from many reviews of published trials that the majority have been carried out with no statistical calculation of the appropriate sample size. Unless the true treatment effect is large, small trials can yield a statistically significant result only if, by chance, the observed difference in the sample is much larger than the real difference.

This section introduces statistical methods for calculating the appropriate sample size for comparing two independent groups of subjects (parallel group design), or for comparing paired observations (paired or crossover design). These methods are not specific to randomized trials, but apply to two group comparisons in general. While there is some artificiality in the approach, it is vastly preferable to the hit and miss approach that is so common. The calculations are based on the principles of hypothesis testing. Sample size calculations for more complicated trials, including sequential trials, will require statistical assistance, as will those where the main outcome of interest is survival time.


### 15.3.2 Sample size, hypothesis tests and power

We can use the power of a hypothesis test to calculate the appropriate sample size for a clinical trial if we can specify the smallest true difference between the treatments that would be clinically valuable. It is this requirement that is somewhat artificial and difficult to define. In practice, however, it is usually possible to specify the degree of benefit that the new treatment would need to have over the old one for it to be a worthwhile treatment.

The main idea behind the sample size calculations is to have a high chance of detecting, as statistically significant, a worthwhile effect if it exists, and thus to be reasonably sure that no such benefit exists if it is not found in the trial. The greater the power of the study, the more sure we

can be, but greater power requires a larger sample, as we will see. It is common to require a power of between  $80\%$  and  $90\%$ . In effect, we try to make clinical importance and statistical significance agree, and thus reduce problems of interpretation.

The necessary sample size is usually obtained from complicated formulae or there are extensive tables available (Machin and Campbell, 1987), but it is much simpler to use a graphical method. Figure 15.2 shows a nomogram that can be used to calculate the appropriate sample size for all the situations considered in this chapter. It is simple to use and has the added

![](../images/15_Clinical_trials/img2.jpg)  
Figure 15.2 Nomogram for calculating sample size or power (reproduced from Altman, 1982b, with permission).

advantage of being equally easy to use in reverse for determining the power of a study of given sample size.

I shall first consider the case where we intend to have two groups of equal size. The nomogram can be used, however, for unequal sample sizes, as I shall show later. All of the sample size calculations are based on the quantity known as the standardized difference. This is calculated in a different way for continuous or categorical outcome variables, but in principle it is based in each case on the ratio of the difference of interest to the standard deviation of the observations. In other words, we express the difference of interest as a multiple of the standard deviation. As we would expect, the smaller this ratio is the larger the required size of the trial.


#### (a) Continuous data - two independent groups  <!-- 非数字标题，四级 -->

For studies of two independent groups of patients with a continuous outcome measure we need to specify the following quantities:

1. standard deviation of the variable (in each group)  $(s)$ ; 
2. clinically relevant difference  $(\delta)$ ; 
3. the significance level  $(\alpha - \text{two-sided})$ ; 
4. the power  $(1 - \beta)$ ;

and it is assumed that the variable has a Normal distribution in the population. The total sample size is  $N$ .

The standardized difference is calculated simply as the ratio of the difference of interest to the standard deviation, that is  $\delta /s$ . We can use Figure 15.2 to calculate the necessary sample size from the standardized difference for any desired power, choosing either a  $5\%$  or  $1\%$  level of significance.

For example, suppose that we are planning a milk- feeding trial in five- year- old children, to see if a daily supplement of milk for a year will lead to an increased gain in height compared with a control group. (Such a study would in fact be difficult to carry out, for practical and ethical reasons.) We know from published data that at this age children grow on average about  $6 \text{cm}$  in a year, with a standard deviation of  $2 \text{cm}$ . Suppose that the effect of the milk on height gain will be considered important if it is at least  $0.5 \text{cm}$ . We want a high probability of detecting such a difference, so we set the power to be  $0.9 (90\%)$  and choose a  $1\%$  significance level. The standardized difference is  $0.5 / 2.0 = 0.25$ . We can now use Figure 15.2 to calculate the necessary sample size. We 'draw' a straight line from the value  $0.25$  on the scale for the standardized difference to the value  $0.90$  on the scale for power and read off the value for  $N$  on the line corresponding to  $\alpha = 0.01$ , which gives a total sample size of  $900$ , i.e.  $450$  in each group.

There are several possible approaches if no estimate of the standard deviation is available. One way is to start the trial and use the data for the

first patients to estimate the standard deviation and thus the sample size needed. Alternatively, the problem can be redefined in terms of the difference between the proportions above and below some chosen cut- off level, and then use the methods for proportions described below. For example, we may recast a trial of an antihypertensive agent in terms of the difference in the proportion of subjects whose systolic blood pressure is reduced to below  $150 \mathrm{mmHg}$ , rather than a comparison of mean blood pressure. Another possibility is to specify the difference of interest directly in terms of the unknown standard deviation. For example, Guyatt et al. (1987) set up a trial to compare ambroxol and placebo in patients with chronic bronchitis, in which they used a questionnaire to derive a score for severity of symptoms. They did not know the standard deviation of these scores, so specified that they wished to be able to detect a difference between the groups of one standard deviation. The standardized difference was therefore 1.0, and the researchers had avoided the need to specify the standard deviation. All of these solutions involve some degree of subjectivity.

It is easy to calculate the sample size for any combination of input values  $(s, \delta , \alpha , 1 - \beta)$ , and we can always change the sample size by altering the input values. However, it is preferable to decide in advance what the requirements are. While some modest relaxation of these is acceptable, in general if the calculated sample size exceeds what seems practical, then the study can be extended either in time or by running the study at more centres. If it is not possible to get near to the required size of study, then the study may best be abandoned.


#### (b) Continuous data - paired or within person studies  <!-- 非数字标题，四级 -->

The appropriate sample size for paired studies, or within person studies such as crossover trials, is obtained in a very similar way. The main difference is that the standard deviation we use is the standard deviation of the changes expected, which I shall call  $s_d$ . Unfortunately, an estimate of this standard deviation is often not available. If we do have a reasonable estimate of  $s_d$ , we can calculate the standardized difference as  $2\delta /s_d$ , and then use the nomogram as before. (Note the similarity to the formula for independent groups, apart from the multiplier of 2. )


#### (c) Categorical data  <!-- 非数字标题，四级 -->

The nomogram in Figure 15.2 can also be used for studies which have a binary outcome variable. If the outcome variable has more than two categories it is necessary to create a binary variable of interest. For example, if patients are to be assessed as 'improved', 'no change' or 'worse', then the sample size calculation could be based on whether or not the patient has improved.

The calculation of sample size for comparing proportions makes use of

the Normal approximation to the Binomial distribution, discussed in section 8.4.3. It is based on the following information:

1. the expected proportion with the specified outcome in each group  $(p_{1}$  and  $p_{2}$ );

2. the significance level (  $\alpha$  -two-sided);

3. the power  $(1 - \beta)$

The usual way of thinking about specifying  $p_{1}$  and  $p_{2}$  is that previous knowledge should allow us to predict the proportion with the outcome in the control group (say  $p_{1}$ ), and so we need to specify the proportion with the outcome in the experimental group that would represent an important improvement.

Given specified values of  $p_{1}$  and  $p_{2}$  we can calculate the standardized difference as

$$
\frac{p_{1} - p_{2}}{\sqrt{\bar{p}(1 - \bar{p})}}
$$

where  $\bar{p} = (p_{1} + p_{2}) / 2$

For example, suppose we are planning a trial to compare two methods of helping smokers to give up smoking. One group is to be given a new kind of nicotine chewing gum and the other group will receive advice from their doctor and a booklet. On the basis of published evidence we expect that in the advice group  $15\%$  of smokers will remain non- smokers at 6 months. We would be interested in an improvement to  $30\%$  in the group given gum. The proportions to be compared are thus 0.30 and 0.15. Suppose that we want an  $85\%$  probability of detecting such a difference, if it really exists, as statistically significant at the  $5\%$  level. We can use the nomogram to work out the necessary sample size for the trial.

We have  $p_{1} = 0.30$  and  $p_{2} = 0.15$  so  $\bar{p} = (0.30 + 0.15) / 2 = 0.225$ . Using the above formula the standardized difference is given as

$$
\frac{0.30 - 0.15}{\sqrt{0.225(1 - 0.225)}}
$$

or 0.36. We connect the standardized difference of 0.36 to the power of 0.85 in the nomogram in Figure 15.2 and read off the necessary sample size for the trial from the central axis corresponding to a significance level of 0.05, which gives  $N = 280$ . To meet the conditions specified for the trial we thus need to have 140 smokers in each group.


#### (d) Unequal sample size  <!-- 非数字标题，四级 -->

The nomogram can be used for trials in which the sample size in the two groups will be different. Sometimes it is felt desirable or necessary to use unequal (weighted) randomization. As long as the imbalance is not great the loss in power is small.

To use the nomogram to plan a study with unequal groups, we must first calculate  $N$  as if we were using equal groups and then calculate the modified sample size  $N^{\prime}$ . If  $k = n_{1} / n_{2}$  is the ratio of the sample sizes in the two groups, then the required total sample size is

$$
N^{\prime} = N(1 + k)^{2} / 4k
$$

and the two sample sizes are given by  $N^{\prime} / (1 + k)$  and  $k N^{\prime} / (1 + k)$ . So, for example, if we wish to put twice as many subjects on the experimental treatment than on the control, we have  $k = 2$ , and so  $N^{\prime} = 9N / 8$ , a fairly small increase, but for  $k = 3$  we have  $N^{\prime} = 16N / 12$  which is an increase of a third over equal sample sizes.


#### (e) Calculating power  <!-- 非数字标题，四级 -->

The nomogram can be used to calculate the power for a given sample size. We just connect by a straight line the relevant values for the sample size and standardized difference and read off the power of the study on the third scale.

To evaluate the power of a study with unequal sample sizes  $n_{1}$  and  $n_{2}$  we use the 'effective' sample size  $N$ , which is calculated as

$$
N = 4N^{\prime}k / (1 + k)^{2}
$$

where  $k = n_{1} / n_{2}$  and  $N^{\prime} = n_{1} + n_{2}$


#### (f) Getting enough patients  <!-- 非数字标题，四级 -->

Often the sample size calculations reveal a required sample size that exceeds the recruiting capability of a single centre. Rather than carry out a trial that is low in power, it is often worth trying to get other centres to collaborate in a 'multicentre' trial, although there will be organizational difficulties to offset against the benefits of increased sample size.

A further problem is that the expected rate of accrual of patients to a trial can be much less than anticipated by the trial organizers. While this may be partly through over- optimism, it is often largely because of a failure to appreciate the effect of the trial's eligibility criteria. Restricting eligibility may lead to failure to achieve the planned sample size, and thus affect the usefulness of the trial as well as the generalizability of the results. Another factor here is the proportion of eligible patients who refuse to participate. If these rates cannot be reliably estimated, then it is prudent to make an allowance for them when planning the sample size for the trial.

Many of the difficulties can be avoided by having a pilot study, which is also valuable for assessing the quality of the data collection forms, and for checking the logistics of the trial, such as the expected time to examine each patient which affects the number that can be seen in a session. A pilot study may also provide more reliable estimates for use in sample size calculations.


## 15.4 ANALYSIS

The possibility of bias entering a trial at the treatment allocation or during the execution of a trial is well known, but there are also several less well known ways in which bias can arise during the analysis of clinical trial data.

In principle the analysis of clinical trial data should be straightforward, using relatively simple methods outlined in earlier chapters, such as  $t$  tests and  $\chi^{2}$  tests. There are, however, several particular problems that arise in the analysis of clinical trials. I shall first consider the assessment of whether the treatment groups are comparable, then some possible causes of bias, and lastly analyses that are more complicated than simply comparing means or proportions in two groups. A fuller discussion of bias in analysis is given by May et al. (1981).


### 15.4.1 Comparison of entry characteristics

Randomization is a method of eliminating bias in the way that treatments are allocated to patients, but it does not guarantee that the characteristics of the different groups are similar. Methods for trying to keep the groups similar were discussed in section 15.2, but most trials use simple randomization with which it is possible to produce groups with quite different characteristics. For example, in a trial including 36 patients, even when we have 18 subjects in each group, any characteristic that is present in half of the subjects has a  $6\%$  chance of being at least twice as common in one treatment group as in the other. Such imbalance for a prognostic variable could have a marked effect on the results of the trial, and on their credibility.

The first analysis that should be carried out with data from a clinical trial is to summarize the entry or baseline characteristics of the patients in the two groups. It is important to show that the groups are similar with respect to variables that may affect the patient's response. For example, we would usually wish to be happy that the age distribution was similar in the different groups, as many outcomes are age- related. Smoking and stage of disease are other variables often looked at in this way.

The usual way of comparing the baseline characteristics of the groups is by performing hypothesis tests, but a moment's thought should suffice to see that this is unhelpful (Altman, 1985). If the randomization is performed fairly we know that any differences between the two treatment groups must be due to chance. A hypothesis test thus makes no sense. In any case the question at issue is whether the groups differ in a way that might affect their response to treatment, which is clearly a question of clinical importance rather than statistical significance. The only use of hypothesis testing is to judge whether the randomization was performed

fairly, but this will only detect major failures. We expect  $5\%$  of tests to be significant at the  $5\%$  level.

While few trials will give results as close to expectation as that of Ueshima et al. (1987), in which 1 of 20 comparisons was statistically significant at the  $5\%$  level, we do not expect large discrepancies from chance. Collins et al. (1987) gave an example of extreme imbalance that is incompatible with proper randomization. Table 15.4 shows the nodal status of patients allocated to active treatment or control in two centres participating in a randomized trial in early breast cancer. The enormous imbalance in centre 2 can only be interpreted as indicating that the randomization at the centre was improper, and the results from that centre should be ignored.

Table 15.4 Number of patients with different nodal status allocated to treatment or control in two centres participating in a randomized trial in early breast cancer (from Collins et al., 1987)  

<table><tr><td rowspan="2"></td><td colspan="2">Centre 1</td><td colspan="2">Centre 2</td></tr><tr><td>Treatment</td><td>Control</td><td>Treatment</td><td>Control</td></tr><tr><td>Nodal status</td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>62 (61%)</td><td>65 (64%)</td><td>27 (22%)</td><td>63 (50%)</td></tr><tr><td>1-3</td><td>29 (28%)</td><td>28 (28%)</td><td>39 (31%)</td><td>44 (35%)</td></tr><tr><td>4+</td><td>11 (11%)</td><td>7 (7%)</td><td>53 (42%)</td><td>18 (14%)</td></tr><tr><td>Not known</td><td>0 (0%)</td><td>1 (1%)</td><td>6 (5%)</td><td>1 (1%)</td></tr><tr><td>Total</td><td>102 (100%)</td><td>101 (100%)</td><td>125 (100%)</td><td>126 (100%)</td></tr><tr><td></td><td colspan="2">X²= 2.0 on 2 df</td><td colspan="2">X²= 35.4 on 2 df</td></tr><tr><td></td><td colspan="2">P = 0.37</td><td colspan="2">P &amp;lt; 0.00000001</td></tr><tr><td></td><td colspan="4">(Excluding not knowns)</td></tr></table>

Imbalance in a baseline variable is only potentially important, in the sense of affecting the overall result of the trial, if that variable is related to the outcome variable. With proper randomization most variables will be distributed similarly in the different treatment groups. If there are one or more variables with known or suspected prognostic importance that are not very closely balanced we can see whether those variables really are related to the outcome variable, or we can simply adjust for them in the analysis, as discussed in section 15.4.6.


### 15.4.2 Main analysis

The main analysis of a clinical trial is the comparison of the pre- specified outcome measure(s) between the different treatment groups. As already

noted, we can use the simple methods of analysis described in Chapters 9 and 10. For trials of independent groups we can use the two sample  $t$  test, Mann- Whitney  $U$  test, or  $\chi^{2}$  test as appropriate and construct the associated confidence intervals. For paired or matched studies we can use the paired  $t$  test, Wilcoxon paired test, or the McNemar test. Crossover trials require a particular form of analysis, which is described below. There are, however, various possible complicating factors that may need to be considered, which are discussed in the next few sections.

There are, however, various possible complicating factors that may need to be considered, which are discussed in the next few sections.


### 15.4.3 Incomplete data

Data may be incomplete for several reasons. For example, occasional laboratory measurements will be missing because the samples taken were inadequate. It is important to use all the data available, and to specify if any observations are missing. Also, some information may simply not have been recorded. While it may seem reasonable to assume that a particular symptom was not present if it was not recorded, such inferences are in general unsafe and should be made only after careful consideration of the circumstances.

The most important problem with missing information relates to patients who drop out of the study before the end. Withdrawal may be by the clinician, perhaps because of side- effects. Alternatively, the patient may move to another area or just fail to return without reason. Efforts should be made to obtain at least some information regarding the status of these patients at the end of the trial, but some data are still likely to be missing. One possible approach is to assign the most optimistic outcome to all these patients and analyse the data, and then repeat the analysis with the most pessimistic outcome. If the two analyses yield similar results, and results also similar to those from an analysis in which these patients are simply excluded, then we can be fairly confident in the findings. The most common approach is simply to omit all such patients, which is reasonable if the number of withdrawals is not too great, and if the proportion withdrawing is similar in each treatment group. However, if there are many more withdrawals in one treatment group the results of the trial will be compromised, as it is likely that the withdrawals are treatment- related.

If the main outcome measure is the time to some event, such as death or recurrence of disease, then we can use some data for all patients, even those who withdraw (see Chapter 13).


### 15.4.4 Protocol violations

In many trials some patients will not have followed the protocol, either deliberately or accidentally. Included here are patients who actually receive the wrong treatment (i.e. not the one allocated) and patients who do not

take their treatment, known as non- compliers. Also it is sometimes discovered after the trial has begun that a patient was not after all eligible for the trial.

The only safe way to deal with all of these situations is to keep all randomized patients in the trial. The analysis is thus based on the groups as randomized, and is known as an intention to treat analysis. Any other policy towards protocol violations will involve subjective decisions and will thus create an opportunity for bias. It is sometimes useful to perform an additional analysis of only those patients adhering to the protocol, but this cannot be taken as a completely fair comparison. For example, the exclusion of patients who did not comply with the protocol may bias the analysis. The analysis of the groups as randomized must be considered the main analysis.


### 15.4.5 Excluding some events

Sometimes the event of interest, such as myocardial infarction or death, occurs after randomization but before the treatment has commenced, or before it could have had an effect. The exclusion of such patients from the analysis is most unwise and may well lead to controversy. It is desirable to design a trial so that there is a minimal delay between randomization and the start of treatment. Sackett and Gent (1979) discuss this problem at some length.

A similar problem arises when the outcome of interest is death from a specific cause such as cancer. It is often unclear if a death is truly unrelated to the medical condition being treated and so it is generally unwise to exclude deaths from other causes.


### 15.4.6 Adjusting for other variables

If we suspect that the observed differences (imbalance) between the groups at the start of the trial may have affected the outcome we can take account of the imbalance in the analysis. Table 15.1 showed some of the baseline characteristics of patients in a trial where the groups look markedly different. The authors did not adjust for the large differences because none of them is statistically significant. We do not know what effect the imbalance may have had. With small trials it is quite common to have large imbalances that are not statistically significant but which could well be clinically important. (For small trials, therefore, simple randomization is not a good method of treatment allocation.)

Most clinical trials are based on the simple idea of comparing two groups with respect to a single variable of prime interest, for which the statistical analysis is straightforward. We may, however, wish to take one or more other variables into consideration in the analysis. One reason might be that

the two groups were not similar with respect to baseline variables, as in Table 15.1. We can thus perform the analysis with and without adjustment. If the results are similar we can infer that the imbalance was not important, and can quote the simple comparison, but if the results are different we should use the adjusted analysis. Imbalance will only affect the results if the variable is related to the outcome measure. It will not matter if one group is on average much shorter than the other if height is unrelated to response to treatment. Table 15.1 shows imbalance for several variables which we might reasonably suppose would be related to outcome, so an adjusted analysis is strongly indicated. The use of some form of restricted randomization that is designed to give similar groups is thus desirable as it simplifies the subsequent analysis of the data.

Even if the groups had very similar characteristics it may still be desirable to adjust for another variable if we know in advance that the variable is strongly related to prognosis. Age is often such a variable. Adjustment for variables known to affect outcome can improve the power of the trial, although not greatly, by improving the precision with which we estimate the treatment effect. Again the effect of adjustment can be assessed by comparison with the unadjusted analysis. Further discussion is given in Altman (1985).

Adjusting for other variables requires the use of the analysis of covariance or some form of multiple regression analysis, as described in Chapter 12.


### 15.4.7 Multiple outcome measures

I suggested in section 15.2.10 that where possible one outcome measure should be treated as the main focus of attention in the analysis. There may be other outcome measures, and these can be analysed using the same methods, but the findings given less emphasis. If there are genuinely several outcome measures of importance, then the  $\mathbf{P}$  value considered statistically significant should be made smaller than the usual  $5\%$  to keep the risk of a Type I error small. One simple method is to use the Bonferroni correction, in which if there are  $k$  variables being analysed then the  $\mathbf{P}$  values are multiplied by  $k$  (see section 9.8.4).

Smith et al. (1987) reviewed 66 clinical trials published in four major general journals: Lancet, British Medical Journal, New England Journal of Medicine, and the Journal of the American Medical Association. They found that the mean number of outcome measures analysed was 22. Appreciation of the dangers of multiple comparisons was rare. A review of 196 reports of trials of nonsteroidal anti- inflammatory drugs in rheumatoid arthritis (Gøtzsche, 1989) found that over 70 different outcome measures were used, with a median of eight per trial. In only  $6\%$  of trials was a main outcome variable chosen in advance.

Gøtzsche (1989) also highlighted the common error of multiple counting of measurements or side- effects. The 'sampling unit' (unit of investigation) of a clinical trial is the patient, so results should relate to patients rather than, for example, joints or teeth.


### 15.4.8 Changes from baseline

I observed in section 5.2 that a clinical trial is a longitudinal study. Although it is common to take the patients' status at the end of the study period as the outcome of interest, sometimes it is more appropriate to take the change from the pre- treatment, or baseline, measurement as the prime outcome measure. For example, in a trial comparing anti- asthma treatments, the improvements in each individual's lung function would be the focus of attention rather than their lung function at the end of the study. This analysis has the important advantage of removing any differences between the groups with respect to pre- treatment levels of the outcome variable. When changes from baseline are analysed it is misleading to perform separate analyses (either hypothesis tests or confidence intervals) within each treatment group. A better approach is to calculate each patient's change from baseline, and then compare directly the changes in the different groups.


### 15.4.9 Subgroup analyses

There is often interest in identifying which patients do well on a treatment and which do badly. We can answer a question like this by analysing the data separately for subsets of the data. We may, for example, re- do the analysis including only male patients, only patients less than 50, or those with a particular symptom. Subgroup analyses like these pose problems of interpretation similar to those resulting from multiple outcome measures. It is reasonable to carry out a small number of subgroup analyses if these were specified in the protocol, but on no account should the data be analysed in numerous different ways in the hope of discovering some significant comparison. An example of the dangers of searching through multiple subgroups is given by Collins et al. (1987), who showed that in a trial on patients with suspected acute myocardial infarction the benefit of treatment was four times as great for patients born under Scorpio than for patients born under all other signs put together.

In many cases the real question of interest is not whether the difference between the treatments is present in a subgroup of patients, but whether the treatment effect differs among two or more complementary subgroups. Thus, for example, in a placebo- controlled trial we may wish to know if the active treatment is more effective among younger patients than older patients. A common approach is to analyse separately the data for the

younger and older patients and compare the two P values. This analysis makes comparisons between the two groups based on analyses carried out separately within each group, and is not a valid method. (A similar situation was described in the previous section.) The correct approach is to compare the difference between the treatments for the two age groups; in other words we look at the interaction between age and treatment. The possibility of an interaction can be examined within an appropriate multiple regression model, whether the outcome variable is continuous, binary or survival time. I recommend expert advice for this analysis. (See also Pocock, 1983, p. 213. ) Note that this analysis is more like that from an observational study, and so we cannot infer causality from any association.

### 15.4.10 Crossover trials

Crossover trials were described in section 15.2.5. The analysis of a crossover trial will be illustrated using data from a trial comparing nicardipine, a calcium- channel blocker, and placebo in the treatment of Raynaud's phenomenon (Kahan et al., 1987). The data, representing the number of attacks in two weeks, are shown in Table 15.5 separately for the groups having nicardipine followed by placebo and vice versa.

The analysis is simplified by calculating for each subject the difference  $(d_{i})$  and average  $(a_{i})$  of the observations in the two periods, and averaging these for each group as shown in Table 15.5. It is incorrect to ignore the design of the study and just perform a simple comparison of treatments. Before comparing the treatments there are two other tests that should be carried out. The correct analysis consists of three two sample  $t$  tests or Mann- Whitney tests;  $t$  tests are used here. (For categorical data we use  $\chi^{2}$  tests.)

The possibility of a period effect is tested by a two sample  $t$  test to compare the differences between the periods in the two groups of patients. If there was no general tendency for patients to do better in one of the periods we would expect the mean differences between the periods in the two groups to be of the same size but having opposite signs. The test for a period effect is thus a two sample  $t$  test comparing  $\bar{d}_{1}$  with  $- \bar{d}_{2}$ .

We investigate the possibility of a treatment- period interaction by noticing that in the absence of an interaction a patient's average response to the two treatments would be the same regardless of the order in which they were received. The test for interaction is thus a two sample  $t$  test comparing  $\bar{a}_{1}$  with  $\bar{a}_{2}$ .

If there is no period effect and no treatment- period interaction the analysis of a crossover trial is simple. However, it is important to investigate possible problems before carrying out the treatment comparison. Both a marked period effect and a treatment- period interaction are worrying because they mean that the observed magnitude of the treatment

Table 15.5 Results from a randomized double- blind crossover trial comparing nicardipine (N) and placebo (P) in patients with Raynaud's phenomenon (Kahan et al., 1987). The data are the number of attacks in two weeks. There was a one- week wash- out period between the two treatment periods

Group A: Nicardipine followed by placebo  $(n = 10)$  

<table><tr><td></td><td>Period 1 Nicardipine</td><td>Period 2 Placebo</td><td>(1) - (2)</td><td>(1) + (2) 2</td><td>P - N</td></tr><tr><td rowspan="11">Mean</td><td>16</td><td>12</td><td>4</td><td>14</td><td>-4</td></tr><tr><td>26</td><td>19</td><td>7</td><td>22.5</td><td>-7</td></tr><tr><td>8</td><td>20</td><td>-12</td><td>14</td><td>12</td></tr><tr><td>37</td><td>44</td><td>-7</td><td>40.5</td><td>7</td></tr><tr><td>9</td><td>25</td><td>-16</td><td>17</td><td>16</td></tr><tr><td>41</td><td>36</td><td>5</td><td>38.5</td><td>-5</td></tr><tr><td>52</td><td>36</td><td>16</td><td>44</td><td>-16</td></tr><tr><td>10</td><td>11</td><td>-1</td><td>10.5</td><td>1</td></tr><tr><td>11</td><td>20</td><td>-9</td><td>15.5</td><td>9</td></tr><tr><td>30</td><td>27</td><td>3</td><td>28.5</td><td>-3</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mean</td><td>24.0</td><td>25.0</td><td>-1.0 (d1)</td><td>24.5 (d1)</td><td>1.0</td></tr><tr><td>SD</td><td>15.61</td><td>10.84</td><td>9.87</td><td>12.50</td><td>9.87</td></tr></table>

Group B: Placebo followed by nicardipine  $(n = 10)$  

<table><tr><td></td><td>Period 1 Placebo</td><td>Period 2 Nicardipine</td><td>(1) - (2)</td><td>(1) + (2) 2</td><td>P - N</td><td></td></tr><tr><td rowspan="12">Mean</td><td>18</td><td>12</td><td>6</td><td>15</td><td>6</td><td></td></tr><tr><td>12</td><td>4</td><td>8</td><td>8</td><td>8</td><td></td></tr><tr><td>46</td><td>37</td><td>9</td><td>41.5</td><td>9</td><td></td></tr><tr><td>51</td><td>58</td><td>-7</td><td>54.5</td><td>-7</td><td></td></tr><tr><td>28</td><td>2</td><td>26</td><td>15</td><td>26</td><td></td></tr><tr><td>29</td><td>18</td><td>11</td><td>23.5</td><td>11</td><td></td></tr><tr><td>51</td><td>44</td><td>7</td><td>47.5</td><td>7</td><td></td></tr><tr><td>46</td><td>14</td><td>32</td><td>30</td><td>32</td><td></td></tr><tr><td>18</td><td>30</td><td>-12</td><td>24</td><td>-12</td><td></td></tr><tr><td>44</td><td>4</td><td>40</td><td>24</td><td>40</td><td></td></tr><tr><td>Mean</td><td>34.3</td><td>22.3</td><td>12.0 (d2)</td><td>28.3 (d2)</td><td>12.0</td></tr><tr><td>SD</td><td>14.99</td><td>19.14</td><td>16.34</td><td>15.12</td><td>16.34</td></tr></table>

effect depends on the order in which the treatments were given. The latter is a more serious problem because it leads to a biased estimate of the treatment effect. (See also section 15.2.5. )

We can test the treatment effect by performing a one sample  $t$  test on all 20 within subject differences between the two treatments. Because the two crossover groups may not be the same size it is preferable to consider the average effect in the two periods, which is equivalent to performing a two sample  $t$  test to compare  $\vec{d}_{1}$  and  $\vec{d}_{2}$ .

For this example the period effect and treatment- period interaction give  $t = 1.82$  and  $t = 0.613$  respectively, both on 18 degrees of freedom, giving  $\mathbf{P} = 0.09$  and  $\mathbf{P} = 0.55$ . As neither is statistically significant we can go on to evaluate the treatment effect using a further two sample  $t$  test, which gives  $t = 2.154$  on 18 degrees of freedom  $(\mathbf{P} = 0.045)$ . The number of attacks in two weeks on nicardipine was on average 6.5 fewer than during two weeks on placebo, with a  $95\%$  confidence interval from 0.18 to 12.82. Although statistically significant at the  $5\%$  level, the magnitude of the effect of nicardipine is uncertain, reflecting the small sample size.

A problem with the analysis of crossover trials is that the important test for a possible treatment- period interaction is noted for its lack of statistical power. The above analysis is a good example, because Table 15.5 shows that patients in group 1 did nearly as well on placebo as they had on nicardipine, suggesting a long- lasting 'carry- over' effect of the active drug. Patients in group 2 showed a big improvement when they changed from placebo to nicardipine. This apparent interaction is not nearly statistically significant. The data from period 1 taken alone suggest that the true benefit of nicardipine might well be rather greater than indicated by the overall results of the trial.

In contrast Ueshima et al. (1987) found a marginally significant  $(0.05 < \mathbf{P} < 0.10)$  treatment- period interaction in a crossover trial to investigate the possible effect on blood pressure of reducing alcohol intake. They discarded the data from the second period.

A graphical approach is to produce a scatter plot of the difference between the two periods against the average of the two periods, using different symbols to identify the two groups (Clayton and Hills, 1987). Vertical separation of the two groups is an indication of a difference between the treatments. If there is no treatment- period interaction there should be no horizontal difference between the groups, and the data for the two groups should lie symmetrically either side of the line  $y = 0$ , as in Figure 15.3(a). Figure 15.3(b) shows such a plot for the nicardipine trial, indicating both horizontal and vertical differences between the two groups, in line with the results already presented.

A comparison of baseline readings taken at the start of each period can show whether the washout period was successful. For example, Table 15.6

shows baseline data from a randomized crossover trial comparing rifampicin with phenobarbitone for treatment of pruritus in biliary cirrhosis. It is clear that patients in the first group had less severe pruritis at the beginning of the second period than at the start of the study. Thus either

![](../images/15_Clinical_trials/img3.jpg)

Table 15.6 Distribution of pruritus scores, from 0 (mild) to 3 (severe) before each period in a two-period crossover trial (Bachs et al., 1989)  

<table><tr><td rowspan="2"></td><td colspan="4">Pruritus score</td></tr><tr><td>0</td><td>1</td><td>2</td><td>3</td></tr><tr><td>Group 1 (n = 12)</td><td></td><td></td><td></td><td></td></tr><tr><td>Before rifampicin</td><td>0</td><td>2</td><td>5</td><td>5</td></tr><tr><td>Before phenobarbitone*</td><td>3</td><td>3</td><td>1</td><td>4</td></tr><tr><td>Group 2 (n = 10)</td><td></td><td></td><td></td><td></td></tr><tr><td>Before phenobarbitone</td><td>0</td><td>2</td><td>2</td><td>6</td></tr><tr><td>Before rifampicin</td><td>0</td><td>2</td><td>2</td><td>6</td></tr></table>

\*One patient dropped out after period 1.

the pruritus had been improved by the first treatment, so that a crossover trial was inappropriate, or the washout period was too short. In general, it is advantageous to incorporate baseline readings into the analysis, but this makes the analysis more complex.

Crossover trials are particularly vulnerable to the effects of patient withdrawal. If a patient withdraws after the first period they cannot be included in the analysis because they never received the other treatment. The randomized groups are thus compromised when there are withdrawals, especially when these are more common in one group. If there are many withdrawals it may be best to discard the data from the second period.

In a report of a crossover trial it is essential that any withdrawals from the trial are documented, with reasons. Also, the baseline characteristics of the two randomized groups should be described. Although this is routine in parallel group trials, most published reports of crossover trials do not give this information.


## 15.5 INTERPRETATION OF RESULTS


### 15.5.1 Single trials

In most cases the statistical analysis of a clinical trial will be simple, at least with respect to the main outcome measure, perhaps involving just a  $t$  test or a Chi squared test. Interpretation seems straightforward, therefore, but for one difficulty. Inference from a sample to a population relies on the assumption that the trial participants are representative of all such patients. In most trials, however, participants are selected to conform to certain inclusion criteria, so extrapolation of results to other types of patient may not be warranted. For example, most trials of anti- hypertensive agents, such as beta- blocking drugs, are carried out on middle- aged men. Is it

reasonable to assume that the results apply to women too, or to young or very old men? In the absence of any information to the contrary it is common to infer wider applicability of results, but the possibility that different groups would respond differently should be borne in mind. It is because of this possibility that subgroup analyses are carried out, as they may give clues about variation in the effectiveness of a treatment (or side- effects) for different groups of patients. Unfortunately, as indicated above, there is a risk of coming up with a misleading result as a consequence of carrying out several such analyses.


### 15.5.2 All published trials

In many fields there have been several similar clinical trials, and it is natural to want to assess all the evidence at once. The first thing that becomes apparent when looking at the results of a series of clinical trials of the same treatment is that the results vary, sometimes markedly. We would of course expect to see some variation in treatment effect, because of random variation, and should not necessarily be worried by it. The confidence interval for the treatment benefit observed in a single trial gives an idea of the range of treatment benefit likely to be observed in a series of trials of the same size.

A recent development has been a move towards the formal statistical analysis of data from all published trials to get an overall assessment of treatment effectiveness. The analysis is known either as an overview or a meta- analysis (Collins et al., 1987). Overviews have often found a highly significant overall treatment benefit when most of the individual trials did not get a significant result. Again, this is not surprising, as many clinical trials are too small to detect anything other than an unrealistically huge treatment benefit. A common criticism of overviews is that they combine information from trials with different patient characteristics and designs. However, any trial may be considered as one of a series, representing just part of the spectrum of disease (Elwood, 1982), and so a clear picture emerging from an overview will indicate wider generalizability of results than is warranted from a single trial. It is important, though, to assess whether the results differ according to the nature of the trial.

A problem in performing overviews is that they usually are based on all the published trials. There is increasing evidence that medical journals exert publication bias (Begg and Berlin, 1988), perhaps unintentionally, by which it is easier to publish the results of a clinical trial in a journal if the treatment effect was significant than if it was not. Also authors make less effort to publish when the results are not significant. Such bias stems from the widespread but mistaken belief that non- significant results are uninteresting or uninformative or both. Here we see another possible benefit of expressing results as a confidence interval rather than an isolated P value.

Pooling all published trials will magnify any publication bias, and this is the major argument against overviews. However, it is better to use the available information in a systematic way than to rely on subjective assessment of the various trials. Although preferable, it is naturally exceedingly difficult to extract information about unpublished trials, but there have been a few cases where it has been done (for example, by Yusuf et al., 1985).


## 15.6 WRITING UP AND ASSESSING CLINICAL TRIALS

For both writing up a clinical trial and assessing the quality of a published trial it is useful to have a check list of the important issues. Figure 16.2 in the next chapter shows the check list used by statisticians refereeing clinical trials for the British Medical Journal.


### 15.6.1 Writing a paper about a clinical trial

The check list in Figure 16.2 gives some idea of the information that should be included in a report about the design and execution of a trial. Further details can be found in Altman et al. (1989), Chalmers et al. (1981), Gardner et al. (1989), Grant (1989) and Simon and Wittes (1985). It is very important to account for all the patients that were originally randomized, indicating the numbers in each group that were withdrawn. It is better still, especially for large trials, to show what happened to all patients considered for entry to the trial - this can be done by a flow- chart (Hampton, 1981).

The results section should include information about the baseline characteristics of the different groups, especially with respect to known prognostic factors. Some comment on the comparability of the groups is needed - this should not be based on hypothesis testing.

Thereafter the results of between group comparisons should be presented, taking note of the problems of multiple outcome measures and subgroup analyses as discussed above.


### 15.6.2 Assessing published trials

Trials must be judged on the information that is included in the published report. We cannot assume a satisfactory answer to any of the questions on the check list if the information is not given. As Colton (1974, p. 269) noted: 'It is the author's onus to demonstrate that bias did not occur or was unlikely to have arisen'.

Many reviews have shown that the standard of published clinical trials leaves a lot to be desired. Most trials these days are randomized, but not

all are as blind as they could be. Few trials seem to have been planned with regard to the sample size necessary to detect a clinically important treatment benefit. Common problems in reports of trials are the omission of the statistical method used to analyse the data, and failure to include or account for all the patients that were randomized. I discuss the quality of published papers in general more fully in the next chapter.

## EXERCISES

15.1 How would you assess the comparability of the treatment groups at baseline in the trial illustrated in Table 3.5?

15.2 In an open (unblinded) trial of the beta- blocker alprenolol given to patients after myocardial infarction, randomization to alprenolol or the standard treatment was at the time of admission to hospital (Ahlmark and Saetre, 1976). The start of medication was two weeks after admission, by which time  $60\%$  of the original 393 patients had been withdrawn from the trial. Reasons for withdrawal were mostly death, non- confirmation of myocardial infarction or contraindication for the beta- blocker. Of the 162 patients actually treated, 69 received alprenolol and 93 the control treatment.

(a) Why might the numbers of withdrawals have been different in the two groups?

(b) Is a hypothesis test to compare the proportions withdrawn in the two groups of any value?

(c) How could this problem have been avoided?

15.3 Thirteen patients with chronic skin disorders were studied in an open trial to assess the effect of aspirin on pruritus. No treatment was given on nights one or two, aspirin  $(900~\mathrm{mg})$  was given on the third and fourth nights, and no treatment was given on the fifth and sixth nights. On each night scratching was measured by a limb movement meter, and an assessment of itching was made by the patient the following morning using a  $10~\mathrm{cm}$  linear analogue scale. For each patient the data were averaged over each two day period. The results were presented in the paper (Daly and Shuster, 1986) as means (SE).

<table><tr><td></td><td>Before treatment</td><td>During treatment</td><td>After treatment</td></tr><tr><td>Linear analogue scale (cm)</td><td>3.5 (0.5)</td><td>3.3 (0.4)</td><td>3.1 (0.5)</td></tr><tr><td>Nocturnal limb movements</td><td>46.3 (10.0)</td><td>38.8 (7.4)</td><td>38.7 (5.5)</td></tr></table>

The complete comment on this table was as follows: 'There was interindividual and intraindividual variation in itch and scratch throughout the study, but no consistent change in either measurement was recorded during aspirin treatment.'

(a) Comment on the design of this study.

(b) What can you say about the distribution across patients of the nocturnal limb movements score?

(c) How were the data analysed? How should the data have been analysed?

(d) How should the results have been presented?

(e) The authors stated: 'We found no effect of oral aspirin... We can therefore exclude the possibility that therapeutic doses of aspirin have a central effect on pruritus.' Why is this conclusion incorrect?

15.4 The table below shows the systolic blood pressures of 16 patients before and after one week's treatment with captopril or placebo. The data are from a randomized controlled trial carried out on insulin- dependent diabetic patients with nephropathy (Hommel et al., 1986).

<table><tr><td rowspan="2"></td><td colspan="2">Captopril</td><td colspan="2">Placebo</td></tr><tr><td>Baseline</td><td>After 1 week</td><td>Baseline</td><td>After 1 week</td></tr><tr><td>1</td><td>147</td><td>137</td><td>1</td><td>133</td></tr><tr><td>2</td><td>129</td><td>120</td><td>2</td><td>129</td></tr><tr><td>3</td><td>158</td><td>141</td><td>3</td><td>152</td></tr><tr><td>4</td><td>164</td><td>137</td><td>4</td><td>161</td></tr><tr><td>5</td><td>134</td><td>140</td><td>5</td><td>154</td></tr><tr><td>6</td><td>155</td><td>144</td><td>6</td><td>141</td></tr><tr><td>7</td><td>151</td><td>134</td><td>7</td><td>156</td></tr><tr><td>8</td><td>141</td><td>123</td><td></td><td></td></tr><tr><td>9</td><td>153</td><td>142</td><td></td><td></td></tr></table>

The authors performed paired  $t$  tests on the data in each group. They found a significant drop in blood pressure in the captopril group, but the change in the placebo group was not significant. They concluded that 'captopril represents a valuable new drug for treating hypertension in diabetics dependent on insulin with nephropathy'.

(a) What is wrong with their analysis, and their interpretation of the results?

(b) Perform a correct analysis. What conclusions can be drawn about the effect of captopril on hypertension?

15.5 A randomized placebo- controlled trial was carried out to investigate

the capacity of aspirin to prevent pregnancy- induced hypertension and preeclamptic toxaemia (Schiff et al., 1989). The study was carried out on women at high risk of these conditions.

(a) How many patients would have been needed to have a  $80\%$  power of detecting as significant  $(\mathbf{P}< 0.05)$  a reduction of one-third in the risk of hypertension from  $30\%$  to  $20\%$ ?

(b) The actual sample size was 65. What was the power of the study to detect the difference considered in 
(a)?

(c) What reduction in risk of hypertension did the study have an  $80\%$  power to detect?

(d) The observed rates of hypertension were  $\frac{11}{31}$ $(36\%)$  in the placebo group and  $\frac{4}{34}$ $(12\%)$  in the aspirin group; the authors quoted  $\mathbf{P} = 0.024$  for this difference. They also quoted a relative risk (RR) of hypertension of 0.33 among the aspirin group. Calculate a  $95\%$  confidence interval for the RR, and comment on the authors' conclusion that large scale clinical trials are needed. (See also problem 10.7.)