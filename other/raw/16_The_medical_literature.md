# 16 The medical literature

One does feel that statistical techniques both of design and analysis are sometimes adopted rather as rituals designed to assuage the last holders of absolute power (editors of journals) and perhaps also regulatory agencies, and not because the techniques are appreciated to be scientifically important.

Cox (1983)

## 16.1 INTRODUCTION

During this century clinical research has grown enormously as has its influence on clinical practice. Publication of research results, especially in a leading journal, will rapidly disseminate those findings all over the world. A paper in a peer- reviewed journal implies that the research is both scientifically sound and clinically worthwhile - it bestows both credibility and respectability on the work. This would be fine if all published papers were scientifically sound but, regrettably, the standard of research leaves much to be desired from the statistical point of view. Examples of substandard design and incorrect analysis can be seen in almost any issue of any medical journal.

The importance of sound design and analysis cannot be overemphasized. Clearly the conclusions from a study must rely on the methods having been correct. If the conclusions are unreliable because of faulty methodology, then the study cannot be clinically worthwhile. Worse, it may be clinically harmful by reason of the conclusions being misleading, and a clinically harmful study is surely unethical.

Thus, with some diffidence, in 1980 I suggested that the misuse of statistics was unethical (Altman, 1982a), a view which has subsequently been widely endorsed but never challenged. The ethical implications of substandard research (not just statistical errors) are:

1. the misuse of patients by exposing them to unjustified risk and inconvenience; 
2. the misuse of resources, including the researchers' time, which could be better employed on more valuable activities; and
3. the consequences of publishing misleading results, which may include the carrying out of unnecessary further work.

In the extreme there may be a direct effect on patient care. In particular, there have been several examples of treatments that were widely used on the basis of promising results from uncontrolled studies, but were later shown by randomized trials to be ineffective (see section 15.2.1). Likewise, conflicting results from epidemiological studies may relate to methodological differences.

These remarks lead to three inevitable conclusions. First it behoves the researcher to take the greatest care in planning, executing, analysing and interpreting research. Second, care is needed too in reading and interpret. ing the research results from other investigators who may have disregarded the first point. As Albert (1981) has said: 'One of the most important skills a physician should have is the ability to critically analyse original contributions to the medical literature.' Third, the standard of statistics in published papers can be influenced by the editorial and refereeing policy of medical journals.

In this final chapter I shall briefly consider the growth of statistics within medical research, summarize the findings of reviews of the quality of statistics in published papers, consider the role of medical journals in improving the quality, and give guidance on reading and writing scientific papers.

## 16.2 THE GROWTH OF STATISTICS IN MEDICAL RESEARCH

It is not possible to pinpoint the introduction of statistical methods into medical research, but with a few notable exceptions we may look to the first quarter of this century. In 1929 a huge paper was published in a physiology journal expounding many of the main principles of statistical analysis and interpretation (Dunn, 1929). By 1937 the correct use of statistical methods in clinical research was considered important enough for the Lancet to publish a series of 15 articles on statistical methods by Austin Bradford Hill. These were quickly republished in book form; that this influential book remains in print 50 years later (Hill, 1984) pays tribute to its quality.

We might consider the modern rise of medical statistics to start either in 1937 or in 1948, when the report of the first well known randomized clinical trial was published. This was the Medical Research Council trial of streptomycin for pulmonary tuberculosis (Medical Research Council, 1948), in which Bradford Hill was a key influence. However, in general the introduction of statistics into medical research was slow. In 1954 the British Medical Journal reported a debate held by the Royal Statistical Society's 'Study Circle on Medical Statistics' (Anon, 1954). The motion was 'This house should welcome the growing influence of statistics in all branches of medicine'. The opposite of the motion made the remarkable observation that medicine was an art and statistics was a science, so statistics was out of place in medicine. More surprising, considering the forum, is that the motion was carried by only a narrow majority.

Much has happened since 1954, and statistical methods are now firmly entrenched in medicine. A good idea of the growth of statistics is given by a study of papers published by the journal Pediatrics in the years 1952, 1962, 1972 and 1982 (Hayden, 1983). As Table 16.1 shows, there was a large increase in the proportion of papers using statistical methods of analysis and a ten fold rise in the use of methods beyond simple  $t$  and  $\chi^{2}$  tests and correlation. The change in the latter was especially marked between 1972 and 1982. A similar study of the New England Journal of Medicine showed that  $45\%$  of papers published in 1978- 79 used only simple methods of statistical analysis (Colditz and Emerson, 1985). Another study contrasted the statistical analyses in papers published in Arthritis and Rheumatism in 1967- 68 and 1982 (Felson et al., 1984). As Table 16.2 shows, they found some marked changes between the two periods. Papers published in 1982 contained many more statistical analyses, which may be at least partly due to the availability of computers. The proportion of papers containing statistical errors was much the same, but the nature of the errors had changed considerably.

Table 16.1 Use of statistical procedures in Pediatrics (Hayden, 1983)  

<table><tr><td rowspan="2"></td><td colspan="4">Year</td></tr><tr><td>1952</td><td>1962</td><td>1972</td><td>1982</td></tr><tr><td>Number of papers</td><td>67</td><td>98</td><td>115</td><td>151</td></tr><tr><td>No statistical procedures</td><td>66%</td><td>59%</td><td>45%</td><td>30%</td></tr><tr><td>Statistical procedures other than t, χ² and r</td><td>3%</td><td>5%</td><td>12%</td><td>35%</td></tr></table>

Several authors have carried out a comprehensive study of journals to see which methods are in most frequent use. Table 16.3 shows the methods found in the review of the New England Journal of Medicine in 1978- 79 in decreasing order of use, with the cumulative percentage of all papers that contained only methods that far down the table. We can see that the wide range of techniques listed covers most but not all published papers. The last ten years have seen a further increase in the use of more advanced statistical techniques, so that the proportion of papers which use methods not shown in Table 16.3 is likely to have increased.

Table 16.2 Use of selected statistical methods in Arthritis and Rheumatism in 1967-68 and 1982, and numbers of errors found (Felson et al., 1984)  

<table><tr><td></td><td>1967-68
(n = 47)</td><td>1982
(n = 74)</td></tr><tr><td>Statistical method:</td><td></td><td></td></tr><tr><td>t test</td><td>8 (17%)</td><td>37 (50%)</td></tr><tr><td>Chi squared test</td><td>9 (19%)</td><td>22 (30%)</td></tr><tr><td>Linear regression</td><td>1 (2%)</td><td>18 (24%)</td></tr><tr><td>Multiple statistical tests</td><td>4 (9%)</td><td>30 (41%)</td></tr><tr><td>Error:</td><td></td><td></td></tr><tr><td>Undefined method</td><td>14 (30%)</td><td>7 (9%)</td></tr><tr><td>Inadequate description of measures of location or dispersion</td><td>6 (13%)</td><td>7 (9%)</td></tr><tr><td>Repeated observations treated as independent</td><td>1 (2%)</td><td>4 (5%)</td></tr><tr><td>Two groups compared on &amp;gt; 10 3 (6%) variables at 5% level</td><td>28 (38%)</td><td></td></tr><tr><td>Multiple t tests instead of analysis of variance</td><td>2 (4%)</td><td>18 (24%)</td></tr><tr><td>Chi squared tests used when expected frequencies too small</td><td>3 (6%)</td><td>4 (5%)</td></tr><tr><td>At least one of above errors</td><td>28 (60%)</td><td>49 (66%)</td></tr></table>

There is thus an enormous diversity of statistical methodology in the medical literature. Unfortunately, as shown in the next section, the reliability of the statistical information in published papers is worryingly low. Thus it is essential to be able to assess critically research papers, to which end it is necessary to be familiar with a large range of statistical concepts and methods. Apart from epidemiological methods, all of the topics listed in Table 16.3 are included in this book.

As well as changes in the methods of statistical analysis there has been a simultaneous change in the types of research design used. A review of the design of studies published in the New England Journal of Medicine over a similar period (1946- 76) showed an increase in clinical trials, and in the proportion of trials that were controlled, but also a decrease in cohort studies in favour of cross- sectional studies (Fletcher and Fletcher, 1979). Much is written about clinical trials, but they still represent a small minority, perhaps only  $5\%$ , of all papers published in medical journals.

Table 16.3 The most common statistical techniques in the New England Journal of Medicine in 1978-79 (Emerson and Colditz, 1983)  

<table><tr><td>Technique</td><td>Cumulative % of papers</td></tr><tr><td>1 No statistical methods or descriptive methods only</td><td>58</td></tr><tr><td>2 t tests</td><td>67</td></tr><tr><td>3 Contingency tables (x²)</td><td>73</td></tr><tr><td>4 Non-parametric tests</td><td>75</td></tr><tr><td>5 Epidemiological methods</td><td>77</td></tr><tr><td>6 Pearson correlation (r)</td><td>79</td></tr><tr><td>7 Simple linear regression</td><td>82</td></tr><tr><td>8 Analysis of variance</td><td>84</td></tr><tr><td>9 Transformations</td><td>86</td></tr><tr><td>10 Rank correlation</td><td>87</td></tr><tr><td>11 Life table analysis</td><td>89</td></tr><tr><td>12 Multiple regression</td><td>90</td></tr><tr><td>13 Multiple comparisons</td><td>92</td></tr></table>

## 16.3 STATISTICS IN PUBLISHED PAPERS

It is clear that the misuse of statistical methods has been a problem from the outset. As early as 1932, commenting on changes over the preceding 20 years, Greenwood wrote: 'Medical papers now frequently contain statistical analyses, and sometimes these analyses are correct, but the writers violate quite as often as before, the fundamental principles of statistical or of general logical reasoning' (Greenwood, 1932). In 1950 Hogben wrote 'Less than 1 per cent of research workers clearly apprehend the rationale of statistical techniques they commonly invoke' (Hogben, 1950). A much more recent comment contains the same message: 'It is nearly impossible to read an issue of leading cancer journals without giving rise to serious questions about study design, data collection, definitions of response, determination of results, and the reporting of results' (Hoogstraten, 1984). These assessments were not supported by systematic reviews of the content of published papers, but since the 1960s there have been many such reviews.

### 16.3.1 Reviews of the literature

The earliest comment I know of relating to the quality of statistics in medical journals is that by Dunn (1929), who observed that half of a series

of published papers examined were not acceptable statistically. One of the first modern reviews was by Schor and Karten (1966) who examined 295 papers published in ten medical journals. They considered that  $28\%$  of the papers were statistically acceptable,  $68\%$  were deficient, and  $5\%$  were 'unsalvageable'. The many subsequent reviews of papers published in numerous different general and specialist journals have found a broadly similar picture. It is difficult to summarize these studies because of the wide range of criteria used by the reviewers, but they have typically found that about half of the papers examined included at least one statistical error. It is also hard to say how important these errors are. Certainly many minor errors will have no material bearing on the overall conclusions of a study, but some may lead to major errors of interpretation.

Most reviewers have looked at errors in statistical analysis, but some have looked at errors in design, especially for clinical trials. For example, Tyson et al. (1983) reviewed reports of 86 therapeutic trials in perinatal medicine published in four journals, using predetermined criteria. Their results are summarized in Table 16.4, and show major deficiencies in the papers examined. Some of the missing information may be due to poor reporting rather than bad design, but when reading a paper we cannot assume things that are not stated. For example, if a report of a clinical trial mentions that random allocation was used but offers no further information

Table 16.4 Summary of review of 86 therapeutic trials in perinatal medicine (Tyson et al.,1983)  

<table><tr><td rowspan="2"></td><td colspan="3">% of studies fulfilling criteria</td></tr><tr><td>Yes</td><td>Unclear</td><td>No</td></tr><tr><td>Statement of purpose</td><td>94</td><td>6</td><td>0</td></tr><tr><td>Clearly defined outcome variables</td><td>74</td><td>1</td><td>25</td></tr><tr><td>Planned prospective data collection</td><td>48</td><td>30</td><td>22</td></tr><tr><td>Predetermined sample size (or a sequential trial)</td><td>3</td><td>16</td><td>71</td></tr><tr><td>Sample size specified</td><td>93</td><td>6</td><td>1</td></tr><tr><td>Disease/health status of subjects specified (n = 85)</td><td>51</td><td>20</td><td>29</td></tr><tr><td>Exclusion criteria specified (n = 81)</td><td>46</td><td>9</td><td>45</td></tr><tr><td>Randomization (if feasible) appropriately performed and documented (n = 69)</td><td>9</td><td>12</td><td>79</td></tr><tr><td>Blinding used, or lack of blinding unlikely to have biased results (n = 83)</td><td>49</td><td>47</td><td>4</td></tr><tr><td>Adequate sample size</td><td>15</td><td>44</td><td>41</td></tr><tr><td>Statistical methods identified, appropriately used and interpreted</td><td>26</td><td>0</td><td>74</td></tr><tr><td>Recommendations/conclusions justified</td><td>10</td><td>71</td><td>19</td></tr></table>

about the procedure used, we cannot assume that they really did randomize. Many researchers do not understand what 'random' means. Likewise, we cannot assume that the statistical methods were appropriate when, as is often the case, the methods are not identified. This is why the reviewers felt unable to judge whether the conclusions were justified in nearly three- quarters of the papers examined.

A comprehensive review of about 150 such studies has been carried out recently (Johnson and Altman, 1990; Altman and Johnson, 1990). It provides little evidence that the frequency of errors is diminishing over time, although it is likely that more recent reviewers have taken a harder line over what they considered to be errors.

Statistical errors can occur at any stage of a study: planning, design, execution, analysis, presentation and interpretation. When planning a study, it is possible to make incorrect judgements about the design or sample size if the findings of other published papers are accepted uncritically. However, the other stages of research, from design through to interpretation, are more obvious places where things can go wrong, and I shall consider each in turn. The examples given are by no means comprehensive.

### 16.3.2 Errors in design

Reviews of clinical trials have shown major deficiencies in the reporting of vital information relating to the design and execution of the trial. Perhaps more worryingly, they have also shown that a fair proportion of papers report studies that have used suboptimal design. For example, despite a huge literature urging the highest possible standards of design for clinical trials, studies are still commonly carried out without concurrent controls, or with concurrent but non- randomized controls, and blinding is not used when it could have been. A major worry is that studies with inferior designs are open to bias, and in particular may produce over- optimistic findings. Fletcher and Fletcher (1979) give several examples where conclusions based on weak research designs were later corrected by subsequent well designed studies. All reviews comparing the results of well designed and poorly designed clinical trials of the same treatments have found that the latter obtained larger treatment effects (Altman and Johnson, 1990). If, as is likely, the weaker studies are also smaller, then the effect of publication bias (see section 15.5.2) may be more severe.

The problems are not confined to clinical trials. Reviews of studies evaluating diagnostic tests have similarly been shown to have major deficiencies in design and reporting (Sheps and Schechter, 1984). However, outside the field of clinical trials it is harder to make general statements about the main errors that are made; Chapter 5 gave several examples of potential difficulties.

One reason for some of the problems is that many studies are not actually designed but rather 'happen'. They are based on an analysis of pre- existing data that were collected for some other purpose. While many reports of such studies admit that the study was retrospective, some pretend that the study was prospective, and thus planned, as it looks better to suggest that the idea came before the data. Symptoms of undesigned studies are variation in the treatments and methods of evaluation used, unequal numbers of observations for different subjects, many missing observations, and a general vagueness about what was done and why.

An example of the use of existing data is seen in many studies using fetal ultrasound measurements to develop reference standards. Virtually all published studies are based on the analysis of existing data, so that the number of observations per fetus varies and the measurements are not taken at pre- specified times. While a single routine ultrasound examination in early pregnancy is common, further ultrasound measurements are not usual unless there is some cause for clinical concern. Thus those fetuses that are represented several times in these data sets are likely to be atypical and quite possibly of a different size on average, so that the data are not what they purport to be. Green and Byar (1984) have discussed some of the problems that can arise from the analysis of data from registries rather than data collected for the purpose in hand.

Other design problems were referred to in Chapter 5. Examples are the choice of an inappropriate high risk sample to make inferences about the general population; choice of inappropriate controls in a case- control study; and the volunteer bias that arises when people can choose their treatment. Another example of a potential problem is the 'healthy worker effect', whereby people in employment are healthier than the general population; this needs to be considered in studies of possible adverse effects of industrial exposure to some hazard. Yet another is the use of different observers in a study to compare two alternative methods of measurement (see section 14.2). If each method is used by only one observer there is an inseparability (or 'confounding') of any systematic differences between the observers with any difference between the methods.

These few examples serve only to illustrate the wide variety of possible pitfalls. At the risk of repetition, I shall say again that the best time to seek expert statistical advice is when you are planning a study, so that any flaws of this sort can be spotted and rectified.

Lastly, there is the fundamental problem of having an inadequate sample size. As noted in section 8.5.4, a review by Freiman et al. (1978) showed that many published clinical trials that find a non- significant difference between treatments had little chance of detecting major treatment effects due to small sample sizes. Few published studies report that the sample size was chosen on the basis of power calculations. Indeed, the concept of

sample size calculations seems almost unknown in medical research outside the field of clinical trials, although the same methods are equally applicable to all comparative studies and can be used in planning any investigation. For example, a recent paper has discussed sample size calculations for rheological studies (Stuart et al., 1989).

Even when power calculations have been used to calculate sample size, the supply of subjects may not be as great as anticipated. It is common in clinical trials for the actual recruitment rate to fall far short of that anticipated, partly because of overestimation of the number of eligible subjects and partly because of their unwillingness to enter the trial.

Whereas  $\mathbf{P}$  values can disguise the fact that a study was too small, a very wide confidence interval indicates the lack of useful information - this is one of the arguments in favour of the use of confidence intervals (see section 8.8).

### 16.3.3 Errors in execution

In prospective studies in particular, the collection of data may not go according to plan. Another way of expressing this idea is that the study plan or protocol is not strictly adhered to. Various problems that can occur in clinical trials were discussed in Chapter 15, notably with regard to the correct exclusion of ineligible subjects and ensuring that each patient received the treatment that was allocated to them. It might be thought that simple allocation schemes, such as those based on odd or even numbers, would be less prone to error, but this is not so. In the report of a study that used both odd and even date of operation and odd or even year of birth to allocate subjects to three different types of heart valve prothesis, the authors wrote: 'We found ... that the randomisation procedure was not entirely consistent throughout the study period, partly because of supply difficulties and partly because the odd/even criteria were sometimes misunderstood' (Kuntze et al., 1989). No further details were given. The mention of supply difficulties suggests that there may have been times when not all three devices were available, but we are not told how this was dealt with. The problems with odd and even dates suggest that this apparently simple scheme may indeed be harder to operate correctly than, for example, randomization using prepared envelopes, apart from this being an inferior design for other reasons. (Of course, the allocation system they used was not randomization, as they wrongly claimed.)

Missing data may be the consequence of a failure in the data collection system, for example arising from neglecting to consider what would happen during weekends or holidays. If the data are not examined until the end of the study, by the time that any problems are spotted it will be too late to rectify them.

### 16.3.4 Errors in analysis

Errors in analysis are regrettably common. In earlier chapters of this book I have warned against improper uses of the methods introduced. These warnings are based on knowledge that such misuses are common in medical journals. Thus the following basic errors are frequently made:

1. using methods of analysis when the assumptions are not met; 
2. analysing paired data ignoring the pairing; 
3. failing to take account of ordered categories; 
4. treating multiple observations on one subject as independent; 
5. using multiple paired comparisons instead of an analysis that considers all groups (e.g. analysis of variance); 
6. performing within group analyses and then comparing groups by comparing  $\mathbf{P}$  values or confidence intervals; 
7. quoting confidence intervals that include impossible values.

I have described these errors as 'basic' because they demonstrate a lack of understanding of fundamental statistical concepts. They are not really excusable.

There are other errors, however, which may be equally or even more serious but where the error is more one of logic than technique. Some examples, all discussed in earlier chapters, are:

1. using correlation in method comparison studies; 
2. using correlation to compare two sets of time-related observations; 
3. assessing the comparability of two or more groups by means of hypothesis tests; 
4. evaluating a diagnostic test solely by means of sensitivity and specificity.

There is perhaps rather more excuse for these errors being made as they are rather more subtle, although the errors have been written about many times in medical journals. There is little excuse for journals not detecting them when papers are submitted.

Another unacceptable practice is to base conclusions on a subset of the data. Apart from the fact that investigation of many subsets or subgroups will lead to a high probability that something will turn up (i.e. yield  $\mathbf{P}< 0.05)$  , presentation of a subset analysis as the main finding may distort the picture. An example is given in a clinical trial comparing two chemotherapy regimes in breast cancer patients (Lippman et al., 1984). The overall comparison of time to progression of disease in the two groups was performed by the logrank test and gave  $\mathbf{P} = 0.26$  . However, the authors also compared the time to progression among only those patients who responded to treatment, for whom there was a significant difference between the groups  $(\mathbf{P} = 0.009)$  . Only the latter analysis appears in the summary of the paper, with no mention that there was no overall significant difference. Either the proportion responding to treatment or

time to progression (or survival time) might be considered a suitable end- point for this study, but the comparison must be based on all patients, not a selected subset.

### 16.3.5 Errors in presentation

With presentation of results too there are several common errors that abound in medical journals:

1. using standard errors (or confidence intervals) for descriptive information; 
2. presenting means (or medians) of continuous data without any indication of variability; 
3. presenting the results of a statistical analysis solely as a  $\mathbf{P}$  value.

All but the last of these problems relate equally to numerical and graphical presentation.

#### (a) Numerical precision

One aspect of presentation that is often poor is the numerical precision used to present data and results. Spurious precision adds nothing to a paper and impairs its readability and credibility. It is hard to provide absolute rules, but the following guidelines may help. When presenting summary statistics or the results of analyses, such as means, standard deviations and regression equations, the precision of the original data should be borne in mind. Means should not usually be quoted to more than one further decimal place than the raw data, but standard errors and standard deviations may require one extra decimal place. Percentages do not need to be given to more than one decimal place at most, especially in small samples. If the numerator and denominator are given, as should usually be the case, then there is no reason not to quote percentages to the nearest integer. Test statistics such as  $t$  and  $X^{2}$  do not need to be given to more than two decimal places. Likewise  $\mathbf{P}$  values do not need more than one or two significant digits, and it is not necessary to be specific below, say, 0.0001 (see section 8.10). Other specific advice is given in several earlier chapters.

Some examples of unnecessary (or spurious) precision, all from published papers, are:

$$
\begin{array}{r l}{P}&{=10^{-54}}\\ {P}&{=0.5254}\\ {r}&{=0.99299}\\ {\chi^{2}}&{=0.7264}\\ {\mathrm{~}^{86.95\%}\mathrm{~of~cases}^{\prime}}\end{array}
$$

An example from a regression analysis is the following equation relating

birth weight in kg (BWt) to chest circumference (CC) and mid- arm circumference (AC) (both in cm) (Bhargava et al., 1985)

$$
\mathbf{BWt} = - 3.0983527 + 0.142088\mathbf{CC} + 0.158039\mathbf{AC}
$$

which purports to predict birthweight to the nearest  $\frac{1}{10000} \mathrm{~g}!$  Many such examples may arise from exact transcription from computer output.

Lastly, a common problem found in many reviews of the literature is the use of the  $\pm$  sign after a mean without specifying whether the number after the sign is the standard deviation or standard error. The  $\pm$  usage has been around for many decades, but the ambiguity is so serious that several medical journals, including the British Medical Journal and Lancet, do not now allow its use, although most journals still do. Thus, for example, the phrase 'the mean blood pressure was  $92.4 \pm 7.1 \mathrm{~mm} \mathrm{Hg}$ ' would be changed to 'the mean blood pressure was  $92.4 \mathrm{~mm} \mathrm{Hg}$  (SD 7.1)' (or SE if that were the case). The preferred notation is unambiguous and also avoids the incorrect implications that the standard deviation (or standard error) can be positive or negative and that the range given by mean  $\pm \mathrm{SD}$  (or mean  $\pm \mathrm{SE}$ ) is of especial interest.

#### (b) Graphical presentation

Although some results can be displayed only as a table or only as a graph, in cases where either is possible there is much uncertainty about which is preferable (journals will not allow the author to display the same data both ways). Again, I do not think it is possible to give simple general guidance. Results are given more accurately in tables, but many people find graphs preferable for seeing the message of the data. Graphs are most advantageous when they show data for individual subjects, for example as scatter diagrams or time trends, rather than summary information.

There is not room here for a comprehensive discussion of the dos and don'ts of graphical presentation. There are now several books devoted to the topic, of which that by Tufte (1983) is particularly worth reading. In the context of this section, however, it is worth considering some ways in which graphs can be misleading.

Scatter diagrams are a particularly valuable type of graph, and give meaning to a correlation or regression analysis. Likewise graphs that show all observations within several groups of subjects are far preferable to those showing just summary statistics. The contrast is illustrated by Figures 3.14 and 9.6, which show the same set of data in both styles. Graphs showing only summary statistics, such as means and standard errors (Figure 9.6), are rarely worth the space they occupy.

Common misleading features of graphs are:

1. the lack of a true zero on the vertical axis; 
2. a change of scale in the middle of an axis (especially heinous in a histogram); 3. three- dimensional effects; 4. failure to show coincident points in a scatter diagram; 5. showing a fitted regression line without a scatter diagram of the raw data; 6. superimposing two (or more) graphs with different vertical scales (especially when they do not start at zero); 7. plotting means without any indication of variability.

The last of these problems is common, and yet the addition of standard deviations or standard errors inevitably leads to a cluttered graph. As noted, this type of data may be better in a table. In the case of serial data, there are better approaches, as outlined in section 14.6.

Further discussion of these and many other issues can be found in Tufte (1983), Cleveland (1984) and Wainer (1984).

### 16.3.6 Errors in interpretation

It seems that the majority of errors in the interpretation of statistical analyses relate to hypothesis tests and  $\mathbf{P}$  values. Many of these points have been covered in earlier chapters, but it is worth reiterating here that the  $\mathbf{P}$  value is not, as is commonly wrongly stated, the probability that the observed effect is due to chance, but rather the probability of obtaining the observed effect (or a more unlikely one) when the null hypothesis is true. In other words,  $\mathbf{P}$  assesses how likely it is to observe such an effect in a sample when there is no such difference in the population.

Another false interpretation is the belief that a  $\mathbf{P}$  value of, say, 0.001 implies a stronger effect than  $\mathbf{P} = 0.01$ . While this may be so, the  $\mathbf{P}$  values do not demonstrate it.

Erroneous interpretations of 'significant' and 'not significant'  $\mathbf{P}$  values abound. There is a common belief that the goal of research is a significant result, and consequently that a non- significant result implies that the research was unsuccessful. This attitude is seen in the frequent description of such study results as 'positive' and 'negative' respectively, and in the awful description of the latter as having 'failed to reach statistical significance'. An example of the contortion that this may lead to is the description of a result with  $\mathbf{P} = 0.05$  as 'probably significant'. Statistical significance is often used as the sole basis of the interpretation. Thus any significant effect, however small or implausible, is taken as real, and any non- significant effect is taken as indicating that there is 'no difference'. To use statistics in this way is to abdicate from any constructive thought about one's results.

The increasing use of confidence intervals may reduce the difficulties. Several leading medical journals have carried editorials or articles supporting the use of confidence intervals and some now expect authors to provide them for their main results (Gardner and Altman, 1989a). It is important in comparative studies that confidence intervals are calculated for the difference between groups, not for the results in each group separately.

The other frequent error in interpretation is to equate association and causation. As discussed in several chapters, an observed association does not necessarily imply that there is a causal relation. The only type of study where we can usually be safe in making such an inference is a well- conducted randomized controlled trial, where any difference in outcome may be taken as causally related to the difference in treatment. Otherwise great caution is needed in the interpretation of results, and causation cannot usually be inferred without other types of evidence. When the false inference is allied to a situation where the observed association itself may be spurious, as described in section 11.3, the scope for error is enormous.

Lastly, I must return to the basic idea of sample and population introduced in section 4.3. Most medical research is based on the principle of extrapolating findings from a sample to a population of interest. Clearly this exercise is crucially dependent upon the sample being representative of the population. In theory the sample should be a random one, but this is almost never the case. In practice, therefore, we need some way of assessing whether the sample may be considered representative, and this is usually done by means of describing the characteristics of the subjects in the sample, and sometimes comparing them with the known characteristics of the population. The whole process of statistical inference fails if the sample is not representative. This is why study results are heavily compromised by high dropout or refusal rates.

### 16.3.7 Errors of omission

Many reviews of the literature have included comments on how often important information was omitted. If the methods of analysis or key aspects of the design are not specified we should not assume that valid procedures were used.

Mosteller et al. (1980) examined 132 controlled trials in cancer and found that the method of statistical analysis was specified in only 46 (35%). The much better figure of 85% for clinical trials published in major journals (DerSimonian et al., 1982) still leaves scope for improvement (see also Table 16.2). If the data are paired or come from ordered categories it is important to know whether the methods used were appropriate. If the data are clearly not Normally distributed, we need to be assured that the method of analysis was appropriate. At a more basic level it is often unclear whether standard deviations or standard errors are presented. It is only occasionally possible to reanalyse the data from the information given in a paper and so verify which method was used, but this should not be necessary.

While the broad structure of the design of a study is usually clear from the report, crucial details may be missing. It is often unclear if all the observations were taken from different individuals. Methods of matching groups may be vague, and indeed we must doubt if matching was used when, as is not uncommon, the 'matched' groups are not of the same size. In randomized trials, the method of randomization is frequently omitted. DerSimonian et al. (1992) found that the method of randomization was reported in only  $19\%$  of the 67 trials that they examined. There are two aspects here: the method of generating the random number sequence, and the mechanism for allocating the treatments. Few papers give both of them, and the more important mechanism is rarely given (Altman and Doré, 1990). We cannot be sure that a trial really did use random allocation simply from the use of the word 'randomized' somewhere in the methods section, or even in the title (Kuntze et al., 1989).One way to see that a paper contains the necessary information is to use a checklist. Examples are discussed in section 16.4.

One way to see that a paper contains the necessary information is to use a checklist. Examples are discussed in section 16.4.

### 16.3.8 Consequences of statistical errors

Reviews of the literature typically report that about half of the papers examined contained a statistical error. However, the term 'error' encompasses an enormous variety of deviations from statistical purity and many errors will not be serious. At the trivial end we may not worry unduly about the presentation of some descriptive information as means with no indication of variability. Our reaction to an analysis where the method is not stated may depend on how 'obvious' it is what was done. For example, two by two tables of frequencies will almost certainly have been analysed by a Chi squared test, but there are many options for continuous data, not all of which will be reasonable in any particular case.

A further problem in assessing the typical  $50\%$  error rate is that there is no general agreement on what constitutes an error. On the other hand, there are other problems that are rarely considered in reviews, such as multiple comparisons or selective reporting of only those results that are significant.

In section 16.1 I gave three ethical implications of statistical errors: the misuse of patients, the misuse of resources and the consequences of publishing misleading results. The last of these can work in several ways, and will depend upon the nature of the results:

1. it may prove impossible to get ethics committee approval to carry out further research because a published study has found the experimental treatment beneficial, even though that study was flawed; 
2. other scientists may be led to follow false lines of investigation; 
3. future patients may receive an inferior treatment, either as a direct

consequence of the results of the study or possibly by the delay in the introduction of a truly effective treatment; 4. if the results go unchallenged the researchers may use the same inferior statistical methods in future research, and others may copy them.

The last of these applies whether or not the study reached inappropriate conclusions.

Some of these problems would, of course, be avoided if everyone was equally able to detect the errors, but it is much better if they can be detected by journals and the papers either not published or suitably amended. Unfortunately, almost any paper can get published somewhere, so however much journals continue to extend their statistical refereeing the problems will remain, as will the need for critical analysis.

### 16.3.9 Why are there so many statistical errors in published papers?

Mistakes in published papers can be ascribed to inadequate understanding of statistics by those using the methods, which in turn is due to inadequate statistical education. Undergraduate teaching of statistics can introduce some of the key statistical concepts, but provides inadequate preparation for the requirements of medical research. Several studies have shown that the statistical understanding by doctors of basic statistical methods and ideas is inadequate (Altman and Bland, 1991). If simple ideas are not well understood, we can hardly expect more complex methods to fare better.

Postgraduate courses are more appropriate for in- depth teaching of statistics, but few researchers have attended such a course. Other reasons for the widespread misuse of statistics have been considered by Altman and Bland (1991), and include misleading textbooks and easy access to computer programs. The recent tremendous increase in the availability of computers and statistical software has given wide access to complex methods of analysis, but there has not been an accompanying increase in understanding of those techniques.

Whatever the reasons, it is at best questionable whether ignorance is an adequate defence. Statistical methods of design and analysis are an essential component of sound medical research, and their use requires certain skills no less than the other components of the research. If those skills are not present within the research team they should be acquired by seeking expert advice. We can again find sound advice in this respect in the 1930s, in the editorial accompanying the first of Bradford Hill's *Lancet* articles: 'The time to allow for statistical factors is when an inquiry is being planned, not when it is completed' (Anon, 1937). Sadly many authors, editors, and even ethics committees remain unconvinced about the importance of correct statistics in medical research. Thus it is necessary to read

published papers with some circumspection, even those published in the most illustrious journals. Section 16.4 gives advice on how to do this.

### 16.3.10 Role of the medical journals

It is self- evident that the standard of statistics in published papers could be greatly raised if journals ceased publishing papers containing major errors. Despite evidence that statistical refereeing can improve the standard of published papers, journals have in general been slow to take steps to judge the statistical component of submitted papers. It is still true that most journals' instructions to authors give far more attention to how the references are laid out than to what statistical information is required, and most do not even mention statistics. Like it or not, journals have a responsibility for publishing papers that are, as far as can be judged, scientifically sound. The quality of the medical literature is in their editorial hands.

As few members of the editorial staff of journals are likely to know much more statistics than the authors of papers, the statistical expertise must be obtained through expert refereeing. In a recent survey, 98 editors of medical journals were asked about their policy on statistical refereeing and replies were obtained from 83 (George, 1985). Only  $16\%$  had a policy that guaranteed a statistical review prior to publication, but  $35\%$  had either a statistical consultant or a statistician on the editorial board. There is clearly considerable scope for improvement in this respect (Altman, 1982c), although it must be acknowledged that there are probably not enough medical statisticians to look at all papers submitted to medical journals. Nevertheless, I believe that all journals should endeavour to obtain some statistical input. Further, they should publish their editorial policy regarding statistical refereeing, something which only  $12\%$  of the journals in the above- mentioned survey had done.

It is most unlikely that there will be a great improvement in the short term except perhaps in a few journals, so it is important to be a cautious reader of published papers. You should not accept research findings solely on the basis of the abstract of a paper, but rather should make a careful assessment of the authors' methods. Some guidance is given in the next section.

## 16.4 READING A SCIENTIFIC PAPER

It is enormously helpful when reading a paper to have a list of specific points to be looking out for. As noted by Colton (1974, p. 317), it is impossible to produce a set of questions that it would be appropriate to ask

for every research paper. A partial solution is to have more than one; for example, Gardner et al. (1989) produced two checklists for statistical reviewers, one for general medical studies and one specifically for clinical trials. Other authors have proposed guidelines for reading reports on clinical trials (Simon and Wittes, 1985; Grant, 1989; Reisch et al., 1989), epidemiological studies including case- control studies (Epidemiology Work Group, 1981; Lichtenstein et al., 1987; Bracken, 1989), and evaluation of diagnostic and screening tests (Sheps and Schechter, 1984; Wald and Cuckle, 1989).

I shall follow the idea of having two different checklists for general studies and for clinical trials. Those shown in Figures 16.1 and 16.2 are heavily based on those in Gardner et al. (1989), but incorporate some extensions and clarifications. They were designed to aid the statistical refereeing of papers submitted to the British Medical Journal, but are equally applicable for assessing papers already published.

The questions in Figure 16.1 can be used to assess medical papers other than clinical trials. Most of the questions relate to important aspects of the design of the study and analysis of the data. Figure 16.2 shows a checklist for assessing reports of clinical trials. The questions relate to aspects of design and analysis that were discussed at length in the previous chapter. Aspects of design of particular importance relate to the description of efforts to eliminate the possibility of bias. As noted already, we cannot infer that a procedure was adopted if the relevant information is absent. For example, it is common to see papers describing trials as both randomized and double blind, but in the absence of any information beyond those three words we should not assume that the authors understand those terms. It is quite common to see methods of alternate allocation described as random; it is, of course, quite different and definitely inferior. A much more detailed assessment scheme for clinical trials was proposed by Chalmers et al. (1981).

It will be appreciated that for many of the questions in the checklists there is no unequivocal answer, and assessing a paper thus involves some subjectivity. Nevertheless, the use of these (or other) checklists makes it much easier to assess a paper, partly because it is always harder to detect omissions than errors in what is present.

The questions in the checklists are not equally important. Ideally we would like to see 'Yes' responses for all questions but few papers will achieve this. In practice we should be most concerned about possible bias in the design of the study. Indeed, if the design of the study is unacceptable for some reason, the paper is statistically unacceptable regardless of how the data were analysed. Next we would hope that the analysis was appropriate to the data, and that the conclusions were justified. Aspects of presentation, while not unimportant, are clearly less important than fundamental aspects of methodology.

### STUDY DESIGN

Is the objective of the study sufficiently described? Yes Unclear No

Is the design of the study sufficiently described? Yes Unclear No

Was the design of the study appropriate to the Yes Unclear No objective?

Is the source of the subjects clearly described? Yes Unclear No

Is the method of selection of the subjects clearly Yes Unclear No described (i.e. inclusion and exclusion criteria)?

Was the sample of subjects appropriate with Yes Unclear No regard to the population to which the findings will be referred?

Was the sample size based on pre- study Yes Unclear No considerations of statistical power?

Is the design of the study acceptable? Yes No

### CONDUCT OF STUDY

Was a satisfactory (high) response rate achieved? Yes Unclear No

### ANALYSIS AND PRESENTATION

Is there a statement adequately describing or Yes No referencing all the statistical procedures used?

Were the statistical methods used appropriate for Yes Unclear No the data?

Were they used correctly? Yes Unclear No

Is the presentation of statistical material (tables, Yes No graphical, numerical) satisfactory?

Are sufficient analyses presented? Yes Unclear No

Are confidence intervals given for the main Yes No results?

### OVERALL ASSESSMENT

Are the conclusions drawn from the statistical Yes Unclear No analyses justified?

Is the paper statistically acceptable? Yes No

Figure 16.1 Checklist for assessment of general medical papers.

### STUDY DESIGN

Is the objective of the trial sufficiently Yes Unclear No described?

Is the design of the study sufficiently Yes Unclear No described?

Is there a satisfactory statement of the Yes Unclear No diagnostic criteria for entry into the trial?

Is the source of the subjects clearly Yes Unclear No described?

Were the treatments well defined? Yes Unclear No

Were the treatment groups studied Yes Unclear No concurrently?

Was random allocation to treatment used? Yes Unclear No

Is the method of creating the randomisation Yes Unclear No (e.g. tables of random numbers) described?

Is the mechanism of treatment allocation Yes Unclear No (e.g. sealed envelopes) described?

Was the mechanism of treatment allocation Yes Unclear No designed to eliminate bias?

Was there an acceptably short delay from Yes Unclear No allocation to commencement of treatment?

Was the potential degree of blindness used Yes Unclear No during the trial?

Is there a satisfactory statement of criteria Yes Unclear No for outcome measures?

Are the outcome measures appropriate? Yes Unclear No

Is there a description of a pre- study Yes No calculation of sample size based on considerations of statistical power?

Is the duration of post- treatment follow- up Yes Unclear No stated?

Is the design of the study acceptable? Yes No

### CONDUCT OF STUDY

Were a high proportion of subjects followed Yes Unclear No up?

Did a high proportion of subjects complete Yes Unclear No treatment?

Are drop- outs described separately for each Yes Unclear No treatment group?

Are side- effects of treatment described Yes Unclear No separately for each group?

### ANALYSIS AND PRESENTATION

Is there a statement adequately describing Yes No or referencing all the statistical procedures used?

Are the baseline characteristics of each Yes No group presented adequately?

Were the statistical methods used Yes Unclear No appropriate for the data?

Were they used correctly? Yes Unclear No

Have prognostic factors been adequately Yes Unclear No considered?

Is the presentation of statistical material Yes No (tables, graphical, numerical) satisfactory?

Are sufficient analyses presented? Yes Unclear No

Are confidence intervals given for the main Yes No results?

### OVERALL ASSESSMENT

Are the conclusions drawn from the Yes Unclear No statistical analyses justified?

Is the paper statistically acceptable? Yes No

Figure 16.2 Checklist for assessment of reports of clinical trials.

## 16.5 WRITING A SCIENTIFIC PAPER

Medical journals give authors scant guidance regarding the statistical aspects of papers. Clearly there is much similarity between aspects of reading a paper and those of writing a paper, and the checklists in Figures 16.1 and 16.2 should give a good idea of the type of information that should be included in a paper. A much more comprehensive set of guidelines for authors is given by Altman et al. (1989), covering all aspects of statistics. The basic principle to be adhered to with respect to the statistical aspects of your research is that the methods should be described in sufficient detail to be fully understood, and so that anyone else with access to your raw data could, if desired, reproduce your results. Put more simply, you should describe clearly exactly what was done.

If your research had a useful objective, you have used a sensible design and adequate sample size, you have performed appropriate analyses and drawn reasonable inferences from your findings you should be able to get your paper published in a good journal. The most important aspect is unquestionably the design, because it is always possible to reanalyse your data. While I hope that after reading this book you will be able to carry out sensible analyses of your data, you may take comfort from the following comment:

I cannot recall that during six years as editor of a major peer- reviewed journal I ever turned down a paper solely because statistical computations were in error, but a large proportion of disapprovals resulted from more fundamental problems of statistical design and concept, which can rarely be remedied after the fact.

(Bailar, 1986)

Statistics is a relatively new field, and like medicine is subject to fashion. In recent years there has been a major shift in policy by many leading medical journals towards encouraging or even requiring authors to use confidence intervals when presenting their main results. The methods have been around for many decades, but have only belatedly become accepted in medicine. Perhaps a continuation of this process would be a decline in the use of hypothesis tests and the plethora of  $\mathbf{P}$  values and asterisks that adorn most medical research papers (Evans et al., 1988). I shall not predict such a development, but it seems likely that the next few years will see further changes in the use of statistics in medical journals. My hope is that we will begin to see a widespread reduction in the frequency of important statistical errors, whose presence compromises the integrity of the literature and may, ultimately, lead to adverse consequences for patients. Following the marked effect on the statistical quality of studies evaluating new drugs as a result of requirements of the regulatory authorities, there are signs of a more widespread increase in awareness of the importance of

good design and correct analysis. To this end I hope that in this book I have succeeded in getting across the important concepts that underlie the statistical component of medical research. For example, it is not important to remember the formula for the confidence interval for a proportion (or any formula for that matter) - you can always look it up. It is important to understand what the confidence interval means. More generally it is essential to understand the key statistical concepts underlying study design and statistical inference.

I shall close with some comments of Mainland (1950), an anatomist who subsequently became a professor of statistics too. They are still as relevant as when he wrote them:

Finally, it must be stressed again that, whatever sources of help are found and whatever techniques are employed, the investigator himself has to grasp the principles of statistical reasoning ... modern statistical principles are not something that we can take or leave as we wish, for they comprise the logic of the investigator in all fields, including the field of clinical research.

## EXERCISES

(These problems are not specific to the material in this chapter.)

16.1 If two studies' results yield  $\mathbf{P}< 0.001$  and  $\mathbf{P} = 0.02$ , why is it not necessarily true, as noted in section 16.3.6, that the former has found a stronger effect than the latter?

16.2 If two identical studies yield  $\mathbf{P}< 0.001$  and  $\mathbf{P} = 0.2$ , what are the possible explanations for the large difference?

16.3 The mean and standard deviation of the heights (in cm) of adult men and women in a population are as follows

<table><tr><td></td><td>Mean</td><td>SD</td></tr><tr><td>Men</td><td>179.1</td><td>5.84</td></tr><tr><td>Women</td><td>171.7</td><td>5.75</td></tr></table>

Assuming that for both sexes height has a Normal distribution,

(a) What proportion of women are above the average height of men?

(b) If  $60\%$  of adults are female, what proportion of adults taller than  $182.9 \mathrm{cm}$  (six feet) are women?

16.4 The following table shows age- specific and total annual death rates per 1000 females in England and Wales in 1931- 5 and 1983 (Office of

Population Censuses and Surveys Mortality Statistics).

<table><tr><td rowspan="2">Age</td><td colspan="2">Annual death rates/1000</td></tr><tr><td>1931–5</td><td>1983</td></tr><tr><td>&amp;lt; 1</td><td>54</td><td>9</td></tr><tr><td>1– 4</td><td>6.2</td><td>0.4</td></tr><tr><td>5– 9</td><td>2.1</td><td>0.2</td></tr><tr><td>10–14</td><td>1.4</td><td>0.2</td></tr><tr><td>15–19</td><td>2.2</td><td>0.3</td></tr><tr><td>20–24</td><td>2.8</td><td>0.3</td></tr><tr><td>25–34</td><td>3.1</td><td>0.5</td></tr><tr><td>35–44</td><td>4.3</td><td>1.2</td></tr><tr><td>45–54</td><td>8.0</td><td>3.6</td></tr><tr><td>55–64</td><td>17</td><td>10</td></tr><tr><td>65–74</td><td>43</td><td>24</td></tr><tr><td>75–84</td><td>109</td><td>64</td></tr><tr><td>85+</td><td>245</td><td>176</td></tr><tr><td>All ages</td><td>11.4</td><td>11.4</td></tr></table>

Over the fifty years there was a large decline in the death rate in every age group. What would explain the fact that the overall death rate per 1000 women of all ages was unchanged?

16.5 The following table shows the percentage of five year olds with five or more decayed, missing or filled teeth  $(\% \mathrm{dmft})$  by social class, separately for children who had lived continuously in either an area with fluoridated water or in a nearby non- fluoridated area (Carmichael et al., 1989).

<table><tr><td>Social class</td><td>Fluoridated</td><td>% dmft 
Non-fluoridated</td></tr><tr><td>I-II</td><td>10</td><td>21</td></tr><tr><td>III</td><td>15</td><td>33</td></tr><tr><td>IV-V</td><td>21</td><td>45</td></tr><tr><td>Unclassified</td><td>20</td><td>47</td></tr></table>

(a) Is it reasonable to use a Chi squared test to compare  $\%$  dmft by social class in the two areas?

(b) The actual numbers of children were as follows:

<table><tr><td>Social class</td><td>Fluoridated</td><td>dmft 
Non-fluoridated</td></tr><tr><td>I-II</td><td>12/117</td><td>12/ 56</td></tr><tr><td>III</td><td>26/170</td><td>48/146</td></tr><tr><td>IV-V</td><td>11/ 52</td><td>29/ 64</td></tr><tr><td>Unclassified</td><td>24/118</td><td>49/104</td></tr><tr><td>Total</td><td>73/457</td><td>138/370</td></tr></table>

Calculate a  $95\%$  confidence interval for the difference in  $\%$  dmft in the total groups of children in the two areas.

(c) Is there a significant relation between  $\%$  dmft and social class within each of the two areas?

(d) How might we assess whether the relation is stronger in the non-fluoridated than the fluoridated area?

(e) What is the name for such an effect?

16.6 The systolic blood pressure (SBP) of 50 pregnant women was measured simultaneously in the same arm using intra- arterial measurement (direct) and a sphygmomanometer (indirect) (Raftery and Ward, 1968). Arm circumference and weight were measured because there was some suggestion that they might affect the difference between the two measurements. The data are shown in the following table:

<table><tr><td>Age</td><td>Weight (kg)</td><td>Arm circumference (cm)</td><td>Systolic BP indirect (mm Hg)</td><td>Diff (I-D)</td></tr><tr><td>1</td><td>32</td><td>78.2</td><td>29</td><td>115</td></tr><tr><td>2</td><td>25</td><td>67.8</td><td>25</td><td>122</td></tr><tr><td>3</td><td>35</td><td>71.7</td><td>26</td><td>118</td></tr><tr><td>4</td><td>41</td><td>60.8</td><td>24</td><td>127</td></tr><tr><td>5</td><td>30</td><td>78.7</td><td>33</td><td>110</td></tr><tr><td>6</td><td>29</td><td>87.8</td><td>31</td><td>146</td></tr><tr><td>7</td><td>20</td><td>68.9</td><td>26</td><td>127</td></tr><tr><td>8</td><td>38</td><td>70.5</td><td>25</td><td>126</td></tr><tr><td>9</td><td>31</td><td>68.0</td><td>29</td><td>81</td></tr><tr><td>10</td><td>39</td><td>72.6</td><td>28</td><td>127</td></tr><tr><td>11</td><td>36</td><td>53.3</td><td>31</td><td>127</td></tr><tr><td>12</td><td>23</td><td>53.3</td><td>23</td><td>80</td></tr><tr><td>13</td><td>25</td><td>46.0</td><td>22</td><td>89</td></tr></table>

<table><tr><td>Age</td><td>Weight (kg)</td><td>Arm circumference (cm)</td><td>Systolic BP indirect (mm Hg)</td><td>Diff (I-D)</td></tr><tr><td>14</td><td>35</td><td>65.9</td><td>26</td><td>136</td></tr><tr><td>15</td><td>26</td><td>68.0</td><td>26</td><td>105</td></tr><tr><td>16</td><td>23</td><td>73.0</td><td>29</td><td>99</td></tr><tr><td>17</td><td>19</td><td>65.6</td><td>26</td><td>129</td></tr><tr><td>18</td><td>21</td><td>59.9</td><td>25</td><td>98</td></tr><tr><td>19</td><td>31</td><td>77.8</td><td>29</td><td>115</td></tr><tr><td>20</td><td>30</td><td>82.0</td><td>31</td><td>169</td></tr><tr><td>21</td><td>39</td><td>65.8</td><td>26</td><td>107</td></tr><tr><td>22</td><td>30</td><td>63.5</td><td>25</td><td>166</td></tr><tr><td>23</td><td>35</td><td>63.6</td><td>28</td><td>93</td></tr><tr><td>24</td><td>34</td><td>73.6</td><td>27</td><td>115</td></tr><tr><td>25</td><td>30</td><td>62.1</td><td>25</td><td>93</td></tr><tr><td>26</td><td>26</td><td>81.1</td><td>33</td><td>118</td></tr><tr><td>27</td><td>29</td><td>70.5</td><td>30</td><td>116</td></tr><tr><td>28</td><td>27</td><td>65.8</td><td>26</td><td>111</td></tr><tr><td>29</td><td>19</td><td>77.6</td><td>31</td><td>159</td></tr><tr><td>30</td><td>21</td><td>58.1</td><td>24</td><td>110</td></tr><tr><td>31</td><td>44</td><td>76.2</td><td>24</td><td>93</td></tr><tr><td>32</td><td>20</td><td>58.4</td><td>25</td><td>93</td></tr><tr><td>33</td><td>33</td><td>59.2</td><td>28</td><td>117</td></tr><tr><td>34</td><td>41</td><td>59.8</td><td>27</td><td>120</td></tr><tr><td>35</td><td>28</td><td>54.9</td><td>23</td><td>114</td></tr><tr><td>36</td><td>28</td><td>79.4</td><td>28</td><td>132</td></tr><tr><td>37</td><td>18</td><td>64.9</td><td>26</td><td>157</td></tr><tr><td>38</td><td>18</td><td>67.6</td><td>25</td><td>109</td></tr><tr><td>39</td><td>32</td><td>61.0</td><td>27</td><td>157</td></tr><tr><td>40</td><td>20</td><td>87.0</td><td>43</td><td>126</td></tr><tr><td>41</td><td>21</td><td>51.5</td><td>23</td><td>83</td></tr><tr><td>42</td><td>31</td><td>81.6</td><td>29</td><td>116</td></tr><tr><td>43</td><td>29</td><td>72.1</td><td>31</td><td>158</td></tr><tr><td>44</td><td>21</td><td>95.3</td><td>31</td><td>118</td></tr><tr><td>45</td><td>28</td><td>74.2</td><td>28</td><td>123</td></tr><tr><td>46</td><td>22</td><td>79.6</td><td>27</td><td>154</td></tr><tr><td>47</td><td>20</td><td>70.5</td><td>26</td><td>126</td></tr><tr><td>48</td><td>28</td><td>79.5</td><td>26</td><td>119</td></tr><tr><td>49</td><td>19</td><td>60.9</td><td>24</td><td>73</td></tr><tr><td>50</td><td>20</td><td>77.6</td><td>27</td><td>116</td></tr></table>

(a) Carry out an appropriate analysis to quantify the agreement between the directly and indirectly measured systolic blood pressures.

(b) Using the results from 
(a), for what proportion of women would

the difference between the methods be expected to be within  $10 \mathrm{mmHg}$ ?

(c) Is there any relation between the differences and weight, arm circumference or age?

(d) The above table shows the data in the order in which the women were studied. The following table shows the mean and standard deviation of the indirect-direct differences in systolic blood pressure for the women taken in blocks of 10:

<table><tr><td rowspan="2">Women</td><td colspan="2">Difference in SBP (mm Hg)</td></tr><tr><td>Mean</td><td>SD</td></tr><tr><td>1–10</td><td>-12.3</td><td>14.0</td></tr><tr><td>11–20</td><td>-4.7</td><td>11.1</td></tr><tr><td>21–30</td><td>-5.2</td><td>9.1</td></tr><tr><td>31–40</td><td>-1.8</td><td>14.3</td></tr><tr><td>41–50</td><td>0.0</td><td>11.8</td></tr><tr><td>Total</td><td>-4.8</td><td>12.5</td></tr></table>

What might explain the variation in the mean difference in SBP?

(e) Repeat 
(a) and 
(b) excluding the first ten women and contrast the answers with those obtained for all 50 women.

16.7 As part of a study of acute mountain sickness fifteen members of a climbing expedition to East Africa had plasma aldosterone measurements taken before and after a rapid ascent to  $4300 \mathrm{m}$  (Milledge et al., 1989). The following table shows measurements taken at 09.00 at low and high altitudes over three days, together with a symptom score for acute mountain sickness (AMS) - high values mean worse symptoms.

<table><tr><td rowspan="2">Subject</td><td rowspan="2">AMS score</td><td rowspan="2">Low 
Day 1</td><td colspan="2">Plasma aldosterone (mmol/l)</td></tr><tr><td>High 
Day 2</td><td>High 
Day 3</td></tr><tr><td>1</td><td>1</td><td>68</td><td>151</td><td>188</td></tr><tr><td>2</td><td>1</td><td>153</td><td>49</td><td>77</td></tr><tr><td>3</td><td>7</td><td>110</td><td>141</td><td>95</td></tr><tr><td>4</td><td>9</td><td>238</td><td>286</td><td>143</td></tr><tr><td>5</td><td>11</td><td>204</td><td>242</td><td>84</td></tr><tr><td>6</td><td>11</td><td>141</td><td>263</td><td>63</td></tr><tr><td>7</td><td>11</td><td>183</td><td>233</td><td>121</td></tr></table>

<table><tr><td rowspan="2">Subject</td><td rowspan="2">AMS score</td><td colspan="3">Plasma aldosterone (mmol/l)</td></tr><tr><td>Low 
Day 1</td><td>High 
Day 2</td><td>High 
Day 3</td></tr><tr><td>8</td><td>13</td><td>119</td><td>245</td><td>97</td></tr><tr><td>9</td><td>13</td><td>272</td><td>275</td><td>115</td></tr><tr><td>10</td><td>14</td><td>166</td><td>241</td><td>150</td></tr><tr><td>11</td><td>15</td><td>228</td><td>109</td><td>76</td></tr><tr><td>12</td><td>17</td><td>77</td><td>192</td><td>63</td></tr><tr><td>13</td><td>18</td><td>114</td><td>46</td><td>43</td></tr><tr><td>14</td><td>23</td><td>91</td><td>189</td><td>74</td></tr><tr><td>15</td><td>35</td><td>105</td><td>254</td><td>283</td></tr></table>

(a) Carry out an appropriate analysis of the plasma aldosterone levels on the three days. Which pairs of days are significantly different?

(b) Obtain a  $95\%$  confidence interval for the difference between plasma aldosterone levels on days 1 and 2.

(c) Examine the association between the AMS score and the change in plasma aldosterone between days 1 and 2.

16.8 In response to the criticism of the analysis of a crossover clinical trial of bran biscuits versus placebo biscuits in the diet of patients with irritable bowel syndrome, the first author wrote:

'In studies such as ours, however, given the relatively small numbers, a confidence interval ... is likely to contain zero, be fairly wide and include both positive and negative values. Therefore this is not an appropriate setting for this form of analysis as the result always will be too diffuse to be meaningful'

(Lucey, 1987).

Is this a reasonable argument?